{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation.ipynb\n",
    "\n",
    "\n",
    "## Note: THIS IS NOT THE MOST UP-TO-DATE SCRIPT\n",
    "    To see the most current version, look to \"/media/data/jacob/GitHub/genetic_algorithm/Notebooks/generation.ipynb\"\n",
    "This is a backup version of the main notebook \"generation.ipynb,\" created at ~1030:pm 11-28-2020 by Jacob Rose, since the the notebook was starting to get cluttered and making it difficult to think. This notebook preserves several possible design directions which were causing more complications than tolerable at this stage in the process, but may be worth returning to upon future refactorizations. These mainly include the custom class definitions for Chromosome and ChromosomeOptions, which atm need a simpler data model to work from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "setGPU: Setting GPU to: [0]\n",
      "Initial visible GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "visible GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Successfully set memory_growth=True and limited GPUs visible to tensorflow.\n",
      "\n",
      "Now using GPU(s):\n",
      "['/physical_device:GPU:0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/conda/jacob/envs/pyleaves2.3/lib/python3.7/site-packages/wandb/vendor/graphql-core-1.1/graphql/type/directives.py:55: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  assert isinstance(locations, collections.Iterable), 'Must provide locations for directive.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 28 00:58:01 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN X (Pascal)    Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 23%   21C    P8     7W / 250W |     10MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN X (Pascal)    Off  | 00000000:05:00.0 Off |                  N/A |\n",
      "| 23%   23C    P8     8W / 250W |     10MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  TITAN X (Pascal)    Off  | 00000000:08:00.0 Off |                  N/A |\n",
      "| 23%   24C    P8     8W / 250W |  10367MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  TITAN X (Pascal)    Off  | 00000000:09:00.0 Off |                  N/A |\n",
      "| 23%   29C    P8     8W / 250W |     10MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  TITAN X (Pascal)    Off  | 00000000:84:00.0 Off |                  N/A |\n",
      "| 23%   23C    P8     7W / 250W |     10MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  TITAN X (Pascal)    Off  | 00000000:85:00.0 Off |                  N/A |\n",
      "| 23%   22C    P8     8W / 250W |  10347MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  TITAN X (Pascal)    Off  | 00000000:88:00.0 Off |                  N/A |\n",
      "| 23%   21C    P8     8W / 250W |     10MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  TITAN X (Pascal)    Off  | 00000000:89:00.0 Off |                  N/A |\n",
      "| 23%   21C    P8     8W / 250W |     10MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    2     30717      C   /home/matt/newtorch-env/bin/python3        10357MiB |\n",
      "|    5     22761      C   ...ata/conda/minju/envs/selfsup/bin/python 10337MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# train_ds = data['train'].map(lambda x,y: (resize(x),y)).shuffle(1024).cache().batch(config.batch_size).prefetch(-1)\n",
    "def get_hardest_k_examples(test_dataset, model, k=32):\n",
    "    class_probs = model.predict(test_dataset)\n",
    "    predictions = np.argmax(class_probs, axis=1)\n",
    "    losses = tf.keras.losses.categorical_crossentropy(test_dataset.y, class_probs)\n",
    "    argsort_loss =  np.argsort(losses)\n",
    "\n",
    "    highest_k_losses = np.array(losses)[argsort_loss[-k:]]\n",
    "    hardest_k_examples = test_dataset.x[argsort_loss[-k:]]\n",
    "    true_labels = np.argmax(test_dataset.y[argsort_loss[-k:]], axis=1)\n",
    "\n",
    "    return highest_k_losses, hardest_k_examples, true_labels, predictions\n",
    "        \n",
    "def log_high_loss_examples(test_dataset, model, k=32):\n",
    "    print(f'logging k={k} hardest examples')\n",
    "    losses, hardest_k_examples, true_labels, predictions = get_hardest_k_examples(test_dataset, model, k=k)\n",
    "    wandb.log(\n",
    "        {\"high-loss-examples\":\n",
    "                            [wandb.Image(hard_example, caption = f'true:{label},\\npred:{pred}\\nloss={loss}')\n",
    "                             for hard_example, label, pred, loss in zip(hardest_k_examples, true_labels, predictions, losses)]\n",
    "        })\n",
    "\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pyleaves.utils import set_tf_config\n",
    "set_tf_config(num_gpus=1)\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "# wandb.login()\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPool2D, ReLU, ELU, LeakyReLU, Flatten, Dense, Add, AveragePooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup, CategoryEncoding\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(666)\n",
    "tf.random.set_seed(666)\n",
    "\n",
    "from typing import List, Tuple, Union, Dict, NamedTuple\n",
    "import tensorflow_datasets as tfds\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from tfrecord_utils.img_utils import resize_repeat\n",
    "from boltons.funcutils import partial\n",
    "\n",
    "# import logging\n",
    "# logger = logging.getLogger('')\n",
    "\n",
    "LOG_DIR = '/media/data/jacob/GitHub/evolution_logs'\n",
    "import os\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "from paleoai_data.utils.logging_utils import get_logger\n",
    "logger = get_logger(logdir=LOG_DIR, filename='generation_evolution_logs.log', append=True)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/conda/jacob/envs/pyleaves2.3/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "TFDS_DATASETS = ['plant_village']\n",
    "\n",
    "class ClassLabelEncoder:\n",
    "    def __init__(self, ds_info: tfds.core.dataset_info.DatasetInfo):\n",
    "        self.info = ds_info\n",
    "        self.dataset_name = ds_info.full_name\n",
    "        self.num_samples = ds_info.splits['train'].num_examples\n",
    "        self.num_classes = ds_info.features['label'].num_classes\n",
    "        self.class_list = ds_info.features['label'].names\n",
    "        self._str2int = ds_info.features['label'].str2int\n",
    "        self._int2str = ds_info.features['label'].int2str\n",
    "        \n",
    "    def str2int(self, labels: Union[List[str],Tuple[str]]):\n",
    "        labels = _valid_eager_tensor(labels)\n",
    "        if not isinstance(labels, [list, tuple]):\n",
    "            assert isinstance(labels, str)\n",
    "            labels = [labels]\n",
    "        return [self._str2int(l) for l in labels]\n",
    "    \n",
    "    def int2str(self, labels: Union[List[int],Tuple[int]]):\n",
    "        labels = _valid_eager_tensor(labels)\n",
    "        if not isinstance(labels, [list, tuple]):\n",
    "            assert isinstance(labels, (int, np.int64))\n",
    "            labels = [labels]\n",
    "        return [self._int2str(l) for l in labels]\n",
    "    \n",
    "    def one_hot(self, label: tf.int64):\n",
    "        '''\n",
    "        One-Hot encode integer labels\n",
    "        Use tf.data.Dataset.map(lambda x,y: (x, encoder.one_hot(y))) and pass in individual labels already encoded in int64 format.\n",
    "        '''\n",
    "        return tf.one_hot(label, depth=self.num_classes)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'''Dataset Name: {self.dataset_name}\n",
    "        Num_samples: {self.num_samples}\n",
    "        Num_classes: {self.num_classes}'''\n",
    "    \n",
    "    def _valid_eager_tensor(self, tensor, strict=False):\n",
    "        '''\n",
    "        If tensor IS an EagerTensor, return tensor.numpy(). \n",
    "        if strict==True, and tensor IS NOT an EagerTensor, then raise AssertionError.\n",
    "        if strict==False, and tensor IS NOT an EagerTensor, then return tensor without modification \n",
    "        '''\n",
    "        try:\n",
    "            assert isinstance(labels, tf.python.framework.ops.EagerTensor)\n",
    "        except AssertionError:\n",
    "            if strict:\n",
    "                raise AssertionError(f'Strict EagerTensor requirement failed assertion test in ClassLabelEncoder._valid_eager_tensor method')\n",
    "        labels = labels.numpy()\n",
    "        return labels\n",
    "\n",
    "def load_plant_village_dataset(split=['train'],\n",
    "                               data_dir=None,\n",
    "                               batch_size=None):\n",
    "    \n",
    "    builder = tfds.builder('plant_village', data_dir=data_dir)\n",
    "    ds_info = builder.info\n",
    "    builder.download_and_prepare()\n",
    "\n",
    "    print(f'splits: {split}')\n",
    "    data = builder.as_dataset(split=list(split),\n",
    "                              shuffle_files=True,\n",
    "                              batch_size=batch_size,\n",
    "                              as_supervised=True\n",
    "                              )\n",
    "    \n",
    "    if not isinstance(data, (tuple, list)):\n",
    "        data = {'train':data}\n",
    "    elif len(data)==2:\n",
    "        data = {'train':data[0], 'val':data[1]}\n",
    "    elif len(data)==3:\n",
    "        data = {'train':data[0], 'val':data[1], 'test':data[2]}\n",
    "    \n",
    "    return data, builder\n",
    "\n",
    "def load_tfds_dataset(dataset_name='plant_village', \n",
    "                      split={'train':'train'},\n",
    "                      data_dir=None,\n",
    "                      batch_size=None):\n",
    "    '''\n",
    "    General interface function to properly route users to the correct function for loading their queried dataset from Tensorflow Datasets (TFDS) public data.\n",
    "    '''\n",
    "    assert dataset_name in TFDS_DATASETS\n",
    "    \n",
    "    print(f'Getting the TFDS dataset: {dataset_name}')\n",
    "    if dataset_name == 'plant_village':\n",
    "        return load_plant_village_dataset(split      =split,\n",
    "                                          data_dir   =data_dir,\n",
    "                                          batch_size =batch_size)\n",
    "    else:\n",
    "        raise Exception('Attempted to load dataset from TFDS that we have yet to build an adapter for. Consider building a minimal working prototype by using alternative datasets as a template.')\n",
    "    \n",
    "\n",
    "def get_parse_example_func(target_size, num_classes):\n",
    "    resize = resize_repeat(target_size=tuple(target_size), training=False)\n",
    "    one_hot = partial(tf.one_hot, depth=num_classes)\n",
    "    def _parse_example(x, y):\n",
    "        x = tf.image.convert_image_dtype(x, tf.float32)\n",
    "        x = resize(x)\n",
    "        y = one_hot(y)\n",
    "        return x,y\n",
    "    return _parse_example\n",
    "\n",
    "def preprocess_data(data: tf.data.Dataset, target_size=None, num_classes=None, batch_size=1): #class_encoder=None):\n",
    "    parse_example = get_parse_example_func(target_size=target_size, num_classes=num_classes) #class_encoder=class_encoder)\n",
    "    return data.map(lambda x,y: parse_example(x, y)) \\\n",
    "                .shuffle(1024) \\\n",
    "                .batch(batch_size) \\\n",
    "                .prefetch(-1)\n",
    "\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(data_config):\n",
    "\n",
    "    data, builder = load_tfds_dataset(dataset_name=data_config.load.dataset_name,\n",
    "                                      split=data_config.load.split,\n",
    "                                      data_dir=data_config.load.data_dir)\n",
    "\n",
    "    data_info     = builder.info\n",
    "    class_encoder = ClassLabelEncoder(data_info)\n",
    "    print(class_encoder)\n",
    "#     vocab = class_encoder.class_list\n",
    "    preprocess = partial(preprocess_data,\n",
    "                         batch_size=data_config.preprocess.batch_size,\n",
    "                         target_size=data_config.preprocess.target_size,\n",
    "                         num_classes=class_encoder.num_classes)\n",
    "\n",
    "    data['train'] = preprocess(data=data['train']) #, batch_size=config.batch_size)\n",
    "    data['val'] = preprocess(data=data['val']) #, batch_size=config.batch_size)\n",
    "    data['test'] = preprocess(data=data['test']) #, batch_size=config.batch_size)\n",
    "    \n",
    "    return data, class_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_and_log(dataset_name='plant_village', \n",
    "#                  data_dir    = '/media/data/jacob/tensorflow_datasets'):\n",
    "# dataset_name='plant_village'\n",
    "# data_dir = '/media/data/jacob/tensorflow_datasets'\n",
    "# if True:\n",
    "#     split = {\n",
    "#              'train': 'train[0%:60%]',\n",
    "#              'val': 'train[60%:70%]',\n",
    "#              'test': 'train[70%:100%]'\n",
    "#             }\n",
    "#     with wandb.init(project=\"artifacts-example\", job_type=\"load-data\") as run:\n",
    "#     run   = wandb.init(project=\"artifacts-example\", job_type=\"load-data\")        \n",
    "#     data, builder = load_tfds_dataset(dataset_name=dataset_name,\n",
    "#                                       split=split,\n",
    "#                                       data_dir=data_dir,\n",
    "#                                       batch_size=None)\n",
    "#     data_info     = builder.info\n",
    "#     raw_data      = wandb.Artifact(\n",
    "#                                     f\"{dataset_name}-raw\", type=\"dataset\",\n",
    "                                    \n",
    "#                                     description=\"Raw {plant_village} dataset, split into train/val/test\",\n",
    "#                                     metadata={\"source\": \"keras.datasets.mnist\",\n",
    "#                                               \"sizes\": [len(dataset.x) for dataset in datasets]})\n",
    "# split = {\n",
    "#          'train': 'train[0%:60%]',\n",
    "#          'val': 'train[60%:70%]',\n",
    "#          'test': 'train[70%:100%]'\n",
    "#         }    \n",
    "# data_dir = '/media/data/jacob/tensorflow_datasets'\n",
    "# data, ds_info = load_plant_village_dataset(split=split,\n",
    "#                                          data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and tracking label encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name='plant_village'\n",
    "data_dir = '/media/data/jacob/tensorflow_datasets'\n",
    "\n",
    "exp_config = OmegaConf.create({'seed':756, #237,\n",
    "                               'batch_size':16,\n",
    "                               'input_shape':(224,224,3),\n",
    "                               'output_size':38,\n",
    "                               'epochs_per_organism':3\n",
    "                              })\n",
    "\n",
    "data_config = OmegaConf.create({'load':{},'preprocess':{}})\n",
    "\n",
    "data_config['load'] = {'dataset_name':'plant_village',\n",
    "                       'split':['train[0%:60%]','train[60%:70%]','train[70%:100%]'],\n",
    "                       'data_dir':'/media/data/jacob/tensorflow_datasets'}\n",
    "\n",
    "data_config['preprocess'] = {'batch_size':exp_config.batch_size,\n",
    "                             'target_size':exp_config.input_shape[:2]}\n",
    "\n",
    "organism_config = OmegaConf.create({'input_shape':exp_config.input_shape,\n",
    "                                    'output_size':38,\n",
    "                                    'epochs_per_organism':5})\n",
    "generation_config = OmegaConf.create({\n",
    "                                      'population_size':5,\n",
    "                                      'num_generations_per_phase':3,\n",
    "                                      'fitSurvivalRate': 0.5,\n",
    "                                      'unfitSurvivalProb':0.2,\n",
    "                                      'mutationRate':0.1,\n",
    "                                      'num_phases':5\n",
    "                                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class OrganismConfig(NamedTuple):\n",
    "#     \"\"\"\n",
    "#     The best run found by an hyperparameter search (see :class:`~transformers.Trainer.hyperparameter_search`).\n",
    "#     Parameters:\n",
    "#         run_id (:obj:`str`):\n",
    "#             The id of the best run (if models were saved, the corresponding checkpoint will be in the folder ending\n",
    "#             with run-{run_id}).\n",
    "#         objective (:obj:`float`):\n",
    "#             The objective that was obtained for this run.\n",
    "#         hyperparameters (:obj:`Dict[str, Any]`):\n",
    "#             The hyperparameters picked to get this run.\n",
    "#     \"\"\"\n",
    "    \n",
    "    \n",
    "#     input_shape: Tuple[int]\n",
    "#     output_size: int\n",
    "#     epochs_per_organism: int=5\n",
    "\n",
    "#     run_id: str\n",
    "#     objective: float\n",
    "#     hyperparameters: Dict[str, Any]\n",
    "\n",
    "\n",
    "# def default_compute_objective(metrics: Dict[str, float]) -> float:\n",
    "#     \"\"\"\n",
    "#     The default objective to maximize/minimize when doing an hyperparameter search. It is the evaluation loss if no\n",
    "#     metrics are provided to the :class:`~transformers.Trainer`, the sum of all metrics otherwise.\n",
    "#     Args:\n",
    "#         metrics (:obj:`Dict[str, float]`): The metrics returned by the evaluate method.\n",
    "#     Return:\n",
    "#         :obj:`float`: The objective to minimize or maximize\n",
    "#     \"\"\"\n",
    "#     metrics = copy.deepcopy(metrics)\n",
    "#     loss = metrics.pop(\"eval_loss\", None)\n",
    "#     _ = metrics.pop(\"epoch\", None)\n",
    "#     return loss if len(metrics) == 0 else sum(metrics.values())\n",
    "\n",
    "\n",
    "# def default_hp_space_optuna(trial) -> Dict[str, float]:\n",
    "#     from .integrations import is_optuna_available\n",
    "\n",
    "#     assert is_optuna_available(), \"This function needs Optuna installed: `pip install optuna`\"\n",
    "#     return {\n",
    "#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "#         \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 5),\n",
    "#         \"seed\": trial.suggest_int(\"seed\", 1, 40),\n",
    "#         \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [4, 8, 16, 32, 64]),\n",
    "#     }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data_dir = data_config.load.data_dir\n",
    "# split = data_config.load.split\n",
    "# batch_size = None\n",
    "\n",
    "# builder = tfds.builder('plant_village', data_dir=data_dir)\n",
    "# ds_info = builder.info\n",
    "# builder.download_and_prepare()\n",
    "\n",
    "# # print(f'splits: {split}')\n",
    "\n",
    "# # data = builder.as_dataset(as_supervised=True,\n",
    "# #                           split=split)\n",
    "\n",
    "# # # data = builder.as_dataset(split=split,\n",
    "# # #                           shuffle_files=True,\n",
    "# # #                           batch_size=batch_size,\n",
    "# # #                           as_supervised=True\n",
    "# # #                           )\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO Load dataset info from /media/data/jacob/tensorflow_datasets/plant_village/1.0.2\n",
      "INFO Reusing dataset plant_village (/media/data/jacob/tensorflow_datasets/plant_village/1.0.2)\n",
      "INFO Constructing tf.data.Dataset for split ['train[0%:60%]', 'train[60%:70%]', 'train[70%:100%]'], from /media/data/jacob/tensorflow_datasets/plant_village/1.0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the TFDS dataset: plant_village\n",
      "splits: ['train[0%:60%]', 'train[60%:70%]', 'train[70%:100%]']\n",
      "Dataset Name: plant_village/1.0.2\n",
      "        Num_samples: 54303\n",
      "        Num_classes: 38\n"
     ]
    }
   ],
   "source": [
    "data, class_encoder = load_and_preprocess_data(data_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZRopbXcoeJ6"
   },
   "source": [
    "# Organism\n",
    "An organism contains the following:\n",
    "\n",
    "1. phase - This denotes which phase does the organism belong to\n",
    "2. chromosome - A dictionary of genes (hyperparameters)\n",
    "3. model - The `tf.keras` model corresponding to the chromosome\n",
    "4. prevBestOrganism - The best organism in the previous **phase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/conda/jacob/envs/pyleaves2.3/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "\n",
    "class Params():\n",
    "    \"\"\"Class that loads hyperparameters from a json file.\n",
    "    Example:\n",
    "    ```\n",
    "    params = Params(json_path)\n",
    "    print(params.learning_rate)\n",
    "    params.learning_rate = 0.5  # change the value of learning_rate in params\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, json_path):\n",
    "        self.update(json_path)\n",
    "\n",
    "    @classmethod\n",
    "    def save(self, json_path):\n",
    "        \"\"\"Saves parameters to json file\"\"\"\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(self.__dict__, f, indent=4)\n",
    "\n",
    "    def update(self, json_path):\n",
    "        \"\"\"Loads parameters from json file\"\"\"\n",
    "        with open(json_path) as f:\n",
    "            params = json.load(f)\n",
    "            self.__dict__.update(params)\n",
    "\n",
    "    def update(self, json_path):\n",
    "        \"\"\"Loads parameters from json file\"\"\"\n",
    "        with open(json_path) as f:\n",
    "            params = json.load(f)\n",
    "            self.__dict__.update(params)\n",
    "\n",
    "            \n",
    "    @property\n",
    "    def dict(self):\n",
    "        \"\"\"Gives dict-like access to Params instance by `params.dict['learning_rate']`\"\"\"\n",
    "        return self.__dict__\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class Stateful(object):\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"Returns the current state of this object.\n",
    "        This method is called during `save`.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def set_state(self, state):\n",
    "        \"\"\"Sets the current state of this object.\n",
    "        This method is called during `reload`.\n",
    "        # Arguments:\n",
    "          state: Dict. The state to restore for this object.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self, fname):\n",
    "        \"\"\"Saves this object using `get_state`.\n",
    "        # Arguments:\n",
    "          fname: The file name to save to.\n",
    "        \"\"\"\n",
    "        state = self.get_state()\n",
    "        state_json = json.dumps(state)\n",
    "        with tf.io.gfile.GFile(fname, 'w') as f:\n",
    "            f.write(state_json)\n",
    "        return str(fname)\n",
    "\n",
    "    def reload(self, fname):\n",
    "        \"\"\"Reloads this object using `set_state`.\n",
    "        # Arguments:\n",
    "          fname: The file name to restore from.\n",
    "        \"\"\"\n",
    "        with tf.io.gfile.GFile(fname, 'r') as f:\n",
    "            state_data = f.read()\n",
    "        state = json.loads(state_data)\n",
    "        self.set_state(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from box import Box\n",
    "from bunch import Bunch\n",
    "from pprint import pprint as pp\n",
    "import random\n",
    "\n",
    "class BaseOptions(NamedTuple):\n",
    "\n",
    "    __indent = None\n",
    "    __hparams = \n",
    "    __chromosomes = {}\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        return self.__data\n",
    "    \n",
    "    @property\n",
    "    def chromosomes(self):\n",
    "        return self.__chromosomes\n",
    "        \n",
    "    def keys(self):\n",
    "        return list(vars(self).keys())\n",
    "    \n",
    "    def values(self):\n",
    "        return list(vars(self).values())\n",
    "\n",
    "    def genes(self):\n",
    "        return self.keys()\n",
    "    \n",
    "    def variants(self):\n",
    "        return self.values()\n",
    "    \n",
    "    def __dumps__(self, indent=None):\n",
    "        return json.dumps(self.chromosomes, indent=indent)\n",
    "    \n",
    "    @classmethod\n",
    "    def __loads__(cls, json_string):\n",
    "        loaded = cls()\n",
    "        loaded.__init__(**json.loads(json_string))\n",
    "        return loaded\n",
    "    \n",
    "    @classmethod\n",
    "    def __save__(self, json_path):\n",
    "        \"\"\"Saves parameters to json file\"\"\"\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(self.data, f, indent=4)\n",
    "    \n",
    "    @property\n",
    "    def serialized(self):\n",
    "        '''\n",
    "        Args:\n",
    "            serialize (bool): default=False\n",
    "                Get the JSON-compatible string representation of the ChromosomeOptions instance\n",
    "        '''\n",
    "        return self.__dumps__(indent=self.__indent)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serialized(cls, serialized: str):\n",
    "        return cls.__loads__(serialized)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([f'{k}:\\n\\t{v}' for k,v in self.chromosomes.items()])\n",
    "        \n",
    "    def __hash__(self):\n",
    "        return hash(self.serialized)\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        return hash(self)==hash(other)\n",
    "\n",
    "\n",
    "# class Chromosome(BaseOptions):\n",
    "class BaseChromosome:#(NamedTuple):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 **kwargs):\n",
    "        self.__chromosome = {k:v for k,v in kwargs.items() if k not in ['seed']}\n",
    "#         self.__chromosomes = {k:v for k,v in locals().items() if k!='self' and not k.startswith('__')}\n",
    "\n",
    "    @property\n",
    "    def chromosome(self):\n",
    "        return self.__chromosome\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([f'{k}:\\n\\t{v}' for k,v in self.chromosome.items()])\n",
    "    \n",
    "    \n",
    "class Chromosome(BaseChromosome):#(NamedTuple):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 a_filter_size:     Tuple[int],\n",
    "                 a_include_BN:      bool,\n",
    "                 a_output_channels: int,\n",
    "                 activation_type:   tf.keras.layers.Layer,\n",
    "                 b_filter_size:     Tuple[int],\n",
    "                 b_include_BN:      bool,\n",
    "                 b_output_channels: int,\n",
    "                 include_pool:      bool, \n",
    "                 pool_type:         tf.keras.layers.Layer,\n",
    "                 include_skip:      bool,\n",
    "                 **kwargs):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.__chromosome = {k:v for k,v in locals().items() if k not in ['self', 'kwargs'] and not k.startswith('__')}\n",
    "#         self.__chromosomes = {k:v for k,v in locals().items() if k!='self' and not k.startswith('__')}\n",
    "\n",
    "    @property\n",
    "    def chromosome(self):\n",
    "        return self.__chromosome\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([f'{k}:\\n\\t{v}' for k,v in self.chromosome.items()])\n",
    "\n",
    "    \n",
    "        \n",
    "#     def __validate_args(self, *args, **kwargs):\n",
    "#         validated = {}\n",
    "#         for k,v in kwargs().items():\n",
    "#             if k!='self' and not k.startswith('_'):\n",
    "#                 validated[k] = v\n",
    "#         return validated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# class ChromosomeOptions(NamedTuple):\n",
    "#     \"\"\"\n",
    "#     Container class for encapsulating variable-length lists of potential gene variants (individual hyperparameters).\n",
    "#     To be used as a reservoir from which to sample a complete chromosome made up of 1 variant per gene.\n",
    "\n",
    "#     Gene: The unique identifier of a particular hyperparameter that may reference any of a set of possible variant values.\n",
    "#     Variant: The particular value of a gene. Used to refer to the 1 value for a single chromosome instance, or 1 value from a set of gene options.\n",
    "\n",
    "#     Args:\n",
    "#         NamedTuple ([type]): [description]\n",
    "#     \"\"\"\n",
    "#     a_filter_size:     List[Tuple[int]]            = [(1,1), (3,3), (5,5), (7,7), (9,9)]\n",
    "#     a_include_BN:      List[bool]                  = [True, False]\n",
    "#     a_output_channels: List[int]                   = [8, 16, 32, 64, 128, 256, 512]\n",
    "#     activation_type:   List[tf.keras.layers.Layer] = [ReLU, ELU, LeakyReLU]\n",
    "#     b_filter_size:     List[Tuple[int]]            = [(1,1), (3,3), (5,5), (7,7), (9,9)]\n",
    "#     b_include_BN:      List[bool]                  = [True, False]\n",
    "#     b_output_channels: List[int]                   = [8, 16, 32, 64, 128, 256, 512]\n",
    "#     include_pool:      List[bool]                  = [True, False]\n",
    "#     pool_type:         List[tf.keras.layers.Layer] = [MaxPool2D, AveragePooling2D]\n",
    "#     include_skip:      List[bool]                  = [True, False]\n",
    "        \n",
    "        \n",
    "#     def generate_chromosome(self, seed=None):\n",
    "        \n",
    "\n",
    "# from prodict import Prodict\n",
    "    \n",
    "    \n",
    "# # class ChromosomeOptions(Prodict):\n",
    "# # class ChromosomeOptions(Dict):\n",
    "# class ChromosomeOptions(NamedTuple):\n",
    "#     \"\"\"\n",
    "#     Container class for encapsulating variable-length lists of potential gene variants (individual hyperparameters).\n",
    "#     To be used as a reservoir from which to sample a complete chromosome made up of 1 variant per gene.\n",
    "\n",
    "#     Gene: The unique identifier of a particular hyperparameter that may reference any of a set of possible variant values.\n",
    "#     Variant: The particular value of a gene. Used to refer to the 1 value for a single chromosome instance, or 1 value from a set of gene options.\n",
    "\n",
    "#     Args:\n",
    "#         NamedTuple ([type]): [description]\n",
    "#     \"\"\"\n",
    "#     a_filter_size:     List[Tuple[int]]            = [(1,1), (3,3), (5,5), (7,7), (9,9)]\n",
    "#     a_include_BN:      List[bool]                  = [True, False]\n",
    "#     a_output_channels: List[int]                   = [8, 16, 32, 64, 128, 256, 512]\n",
    "#     activation_type:   List[tf.keras.layers.Layer] = [ReLU, ELU, LeakyReLU]\n",
    "#     b_filter_size:     List[Tuple[int]]            = [(1,1), (3,3), (5,5), (7,7), (9,9)]\n",
    "#     b_include_BN:      List[bool]                  = [True, False]\n",
    "#     b_output_channels: List[int]                   = [8, 16, 32, 64, 128, 256, 512]\n",
    "#     include_pool:      List[bool]                  = [True, False]\n",
    "#     pool_type:         List[tf.keras.layers.Layer] = [MaxPool2D, AveragePooling2D]\n",
    "#     include_skip:      List[bool]                  = [True, False]\n",
    "        \n",
    "#     def keys(self):\n",
    "#         return self._asdict().keys()\n",
    "    \n",
    "#     def values(self):\n",
    "#         return self._asdict().values()\n",
    "    \n",
    "#     def items(self):\n",
    "#         return self._asdict().items()\n",
    "        \n",
    "# #     def generate_chromosome(self, seed=None):\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "# class ChromosomeOptions:\n",
    "\n",
    "#     a_filter_size:     List[Tuple[int]]            = [(1,1), (3,3), (5,5), (7,7), (9,9)]\n",
    "#     a_include_BN:      List[bool]                  = [True, False]\n",
    "#     a_output_channels: List[int]                   = [8, 16, 32, 64, 128, 256, 512]\n",
    "    \n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         self = Bunch(**kwargs)\n",
    "\n",
    "# options = ChromosomeOptions()\n",
    "\n",
    "# dir((options))\n",
    "\n",
    "# # options.__dict__\n",
    "\n",
    "# 'a_output_channels' in options\n",
    "\n",
    "# options.__class__.__dict__.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a_filter_size': [(1, 1), (3, 3), (5, 5), (7, 7), (9, 9)], 'a_include_BN': [True, False], 'a_output_channels': [8, 16, 32, 64, 128, 256, 512], 'activation_type': ['ReLU', 'ELU', 'LeakyReLU'], 'b_filter_size': [(1, 1), (3, 3), (5, 5), (7, 7), (9, 9)], 'b_include_BN': [True, False], 'b_output_channels': [8, 16, 32, 64, 128, 256, 512], 'include_pool': [True, False], 'pool_type': ['MaxPool2D', 'AveragePooling2D'], 'include_skip': [True, False]}\n",
      "{'a_filter_size': [(1, 1), (3, 3), (5, 5), (7, 7), (9, 9)], 'a_include_BN': [True, False], 'a_output_channels': [8, 16, 32, 64, 128, 256, 512], 'activation_type': ['ReLU', 'ELU', 'LeakyReLU'], 'b_filter_size': [(1, 1), (3, 3), (5, 5), (7, 7), (9, 9)], 'b_include_BN': [True, False], 'b_output_channels': [8, 16, 32, 64, 128, 256, 512], 'include_pool': [True, False], 'pool_type': ['MaxPool2D', 'AveragePooling2D'], 'include_skip': [True, False]}\n",
      "{'a_filter_size': [[1, 1], [3, 3], [5, 5], [7, 7], [9, 9]], 'a_include_BN': [True, False], 'a_output_channels': [8, 16, 32, 64, 128, 256, 512], 'activation_type': ['ReLU', 'ELU', 'LeakyReLU'], 'b_filter_size': [[1, 1], [3, 3], [5, 5], [7, 7], [9, 9]], 'b_include_BN': [True, False], 'b_output_channels': [8, 16, 32, 64, 128, 256, 512], 'include_pool': [True, False], 'pool_type': ['MaxPool2D', 'AveragePooling2D'], 'include_skip': [True, False]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "a_filter_size:\n",
       "\t(5, 5)\n",
       "a_include_BN:\n",
       "\tTrue\n",
       "a_output_channels:\n",
       "\t512\n",
       "activation_type:\n",
       "\tReLU\n",
       "b_filter_size:\n",
       "\t(3, 3)\n",
       "b_include_BN:\n",
       "\tFalse\n",
       "b_output_channels:\n",
       "\t128\n",
       "include_pool:\n",
       "\tTrue\n",
       "pool_type:\n",
       "\tMaxPool2D\n",
       "include_skip:\n",
       "\tFalse"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ActivationLayers = Box(ReLU=ReLU, ELU=ELU, LeakyReLU=LeakyReLU)\n",
    "PoolingLayers = Box(MaxPool2D=MaxPool2D, AveragePooling2D=AveragePooling2D)\n",
    "\n",
    "class ChromosomeOptions(BaseOptions): #object):\n",
    "# class ChromosomeOptions(Prodict):\n",
    "# class ChromosomeOptions(Dict):\n",
    "# class ChromosomeOptions(Box):\n",
    "    \"\"\"\n",
    "    Container class for encapsulating variable-length lists of potential gene variants (individual hyperparameters).\n",
    "    To be used as a reservoir from which to sample a complete chromosome made up of 1 variant per gene.\n",
    "    \n",
    "    This should be logged for describing the scope of a given AutoML experiment's hyperparameter search space\n",
    "\n",
    "    Gene: The unique identifier of a particular hyperparameter that may reference any of a set of possible variant values.\n",
    "    Variant: The particular value of a gene. Used to refer to the 1 value for a single chromosome instance, or 1 value from a set of gene options.\n",
    "\n",
    "    Args:\n",
    "        NamedTuple ([type]): [description]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 include_layer:     Optional[bool]              = [True, False]\n",
    "                 a_filter_size:     List[Tuple[int]]            = [(1,1), (3,3), (5,5), (7,7), (9,9)],\n",
    "                 a_include_BN:      List[bool]                  = [True, False],\n",
    "                 a_output_channels: List[int]                   = [8, 16, 32, 64, 128, 256, 512],\n",
    "                 activation_type:   List[str]                   = ['ReLU', 'ELU', 'LeakyReLU'],\n",
    "                 b_filter_size:     List[Tuple[int]]            = [(1,1), (3,3), (5,5), (7,7), (9,9)],\n",
    "                 b_include_BN:      List[bool]                  = [True, False],\n",
    "                 b_output_channels: List[int]                   = [8, 16, 32, 64, 128, 256, 512],\n",
    "                 include_pool:      List[bool]                  = [True, False],\n",
    "                 pool_type:         List[str]                   = ['MaxPool2D', 'AveragePooling2D'],\n",
    "                 include_skip:      List[bool]                  = [True, False],\n",
    "                 **kwargs):\n",
    "#         super().__init__()|\n",
    "#                  activation_type:   List[tf.keras.layers.Layer] = [ReLU, ELU, LeakyReLU],\n",
    "# pool_type:         List[tf.keras.layers.Layer] = [MaxPool2D, AveragePooling2D],\n",
    "        \n",
    "#         self.__chromosomes = {k:v for k,v in locals().items() if k not in ['self']}\n",
    "#         self.__chromosomes = self.__validate_args(**{k:v for k,v in locals().items() if k!='self'})\n",
    "        self.__chromosomes = {k:v for k,v in locals().items() if k not in ['self', 'kwargs'] and not k.startswith('__')}\n",
    "        print(self.__chromosomes)\n",
    "#         print(self.__chromosomes['validated'])#.keys())\n",
    "        \n",
    "        self.__activation_type = [ActivationLayers[act_layer] for act_layer in self.__chromosomes['activation_type']]\n",
    "        self.__pool_type = [PoolingLayers[pool_layer] for pool_layer in self.__chromosomes['pool_type']]\n",
    "        \n",
    "        if 'seed' in kwargs:\n",
    "            self.rng = np.random.default_rng(seed)\n",
    "        else:\n",
    "            self.rng = np.random.default_rng()\n",
    "        \n",
    "        self.__indent = None\n",
    "\n",
    "    def __dumps__(self, indent=None):\n",
    "        return json.dumps(self.chromosomes, indent=indent)\n",
    "    \n",
    "    @classmethod\n",
    "    def __loads__(cls, json_string):\n",
    "        loaded = cls()\n",
    "        loaded.__init__(**json.loads(json_string))\n",
    "        return loaded\n",
    "        \n",
    "        \n",
    "    def __validate_args(self, *args, **kwargs):\n",
    "        validated = {}\n",
    "        for k,v in kwargs.items():\n",
    "            if k!='self' and not k.startswith('_'):\n",
    "                validated[k] = v\n",
    "        return validated\n",
    "    \n",
    "    \n",
    "    def sample_k_variants_from_gene(self, gene: str, k: int=1):\n",
    "        '''\n",
    "        Randomly sample the list of variants corresponding to the key indicated by the first arg, 'gene'. Produce a random sequence of length k, with the default==1.\n",
    "        \n",
    "        Note: If k==1: this automatically returns a single unit from the variants list, which may or may not be a scalar object (e.g. int, str, float)\n",
    "        If k > 1: then the sampled variants will always be returned in a list.\n",
    "        \n",
    "        '''\n",
    "        all_variants = self.chromosomes[gene]\n",
    "        variant_idx = self.rng.integers(low=0, high=len(all_variants), size=k)\n",
    "        sampled_variants = [all_variants[idx] for idx in variant_idx.tolist()]\n",
    "        if k==1:\n",
    "            sampled_variants = sampled_variants[0]\n",
    "        return sampled_variants\n",
    "    \n",
    "    def generate_chromosome(self, phase: int, seed=None):\n",
    "        '''\n",
    "        Primary function for utilizing a ChromosomeOptions object during experimentation.\n",
    "        Running this function will randomly generate a new Chromosome instance for which each genetic variant is randomly sampled from this object's contained data,\n",
    "        in the form of mappings between gene names as keys, and lists of variants as values.\n",
    "        '''\n",
    "        return Chromosome(**{gene:self.sample_k_variants_from_gene(gene) for gene in self.chromosomes.keys()})\n",
    "    \n",
    "    def generate_k_chromosomes(self, k: int=1, seed=None):\n",
    "        return [self.generate_chromosome(seed=seed) for _ in range(k)]\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def chromosomes(self):\n",
    "        return self.__chromosomes\n",
    "        \n",
    "    if phase == 0:\n",
    "        return {\n",
    "        'a_filter_size': options_phase0['a_filter_size'][np.random.randint(len(options_phase0['a_filter_size']))],\n",
    "        'a_include_BN': options_phase0['a_include_BN'][np.random.randint(len(options_phase0['a_include_BN']))],\n",
    "        'a_output_channels': options_phase0['a_output_channels'][np.random.randint(len(options_phase0['a_output_channels']))],\n",
    "        'activation_type': options_phase0['activation_type'][np.random.randint(len(options_phase0['activation_type']))],\n",
    "        'b_filter_size': options_phase0['b_filter_size'][np.random.randint(len(options_phase0['b_filter_size']))],\n",
    "        'b_include_BN': options_phase0['b_include_BN'][np.random.randint(len(options_phase0['b_include_BN']))],\n",
    "        'b_output_channels': options_phase0['b_output_channels'][np.random.randint(len(options_phase0['b_output_channels']))],\n",
    "        'include_pool': options_phase0['include_pool'][np.random.randint(len(options_phase0['include_pool']))],\n",
    "        'pool_type': options_phase0['pool_type'][np.random.randint(len(options_phase0['pool_type']))],\n",
    "        'include_skip': options_phase0['include_skip'][np.random.randint(len(options_phase0['include_skip']))]\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "        'a_filter_size': options['a_filter_size'][np.random.randint(len(options['a_filter_size']))],\n",
    "        'a_include_BN': options['a_include_BN'][np.random.randint(len(options['a_include_BN']))],\n",
    "        'a_output_channels': options['a_output_channels'][np.random.randint(len(options['a_output_channels']))],\n",
    "        'b_filter_size': options['b_filter_size'][np.random.randint(len(options['b_filter_size']))],\n",
    "        'b_include_BN': options['b_include_BN'][np.random.randint(len(options['b_include_BN']))],\n",
    "        'b_output_channels': options['b_output_channels'][np.random.randint(len(options['b_output_channels']))],\n",
    "        'include_pool': options['include_pool'][np.random.randint(len(options['include_pool']))],\n",
    "        'pool_type': options['pool_type'][np.random.randint(len(options['pool_type']))],\n",
    "        'include_layer': options['include_layer'][np.random.randint(len(options['include_layer']))],\n",
    "        'include_skip': options['include_skip'][np.random.randint(len(options['include_skip']))]\n",
    "        }\n",
    "options = ChromosomeOptions()\n",
    "new_options = options.generate_chromosome()\n",
    "opt_str = options.serialized\n",
    "loaded = ChromosomeOptions.from_serialized(opt_str)\n",
    "options==loaded\n",
    "new_options\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Trial(stateful.Stateful):\n",
    "\n",
    "    def __init__(self,\n",
    "                 hyperparameters,\n",
    "                 trial_id=None,\n",
    "                 status=TrialStatus.RUNNING):\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.trial_id = generate_trial_id() if trial_id is None else trial_id\n",
    "\n",
    "        self.metrics = metrics_tracking.MetricsTracker()\n",
    "        self.score = None\n",
    "        self.best_step = None\n",
    "        self.status = status\n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"Displays a summary of this Trial.\"\"\"\n",
    "        print('Trial summary')\n",
    "\n",
    "        print('Hyperparameters:')\n",
    "        self.display_hyperparameters()\n",
    "\n",
    "        if self.score is not None:\n",
    "            print('Score: {}'.format(self.score))\n",
    "\n",
    "    def display_hyperparameters(self):\n",
    "        if self.hyperparameters.values:\n",
    "            for hp, value in self.hyperparameters.values.items():\n",
    "                print(hp + ':', value)\n",
    "        else:\n",
    "            print('default configuration')\n",
    "\n",
    "    def get_state(self):\n",
    "        return {\n",
    "            'trial_id': self.trial_id,\n",
    "            'hyperparameters': self.hyperparameters.get_config(),\n",
    "            'metrics': self.metrics.get_config(),\n",
    "            'score': self.score,\n",
    "            'best_step': self.best_step,\n",
    "            'status': self.status\n",
    "        }\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self.trial_id = state['trial_id']\n",
    "        hp = hp_module.HyperParameters.from_config(\n",
    "            state['hyperparameters']\n",
    "        )\n",
    "        self.hyperparameters = hp\n",
    "        self.metrics = metrics_tracking.MetricsTracker.from_config(state['metrics'])\n",
    "        self.score = state['score']\n",
    "        self.best_step = state['best_step']\n",
    "        self.status = state['status']\n",
    "\n",
    "    @classmethod\n",
    "    def from_state(cls, state):\n",
    "        trial = cls(hyperparameters=None)\n",
    "        trial.set_state(state)\n",
    "        return trial\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, fname):\n",
    "        with tf.io.gfile.GFile(fname, 'r') as f:\n",
    "            state_data = f.read()\n",
    "        return cls.from_state(state_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. INTERESTING REFACTOR IDEA:\n",
    "    TODO: Refactor chromosome structure to standardize the configuration options for repeated model structures\n",
    "    ### (3 AM 11/27/20)\n",
    "\n",
    "    e.g. Create a separate ConvOptions(NamedTuple) class to contain all 3 options:\n",
    "        filter_size\n",
    "    include_BN\n",
    "    output_channels\n",
    "\n",
    "    Then in each \"ChromosomeOptions\" (consider making each of those a chromosome, and upgrading what's now a chromosome to a full Genome)\n",
    "    store a separate ConvOptions for layer a and layer b, separately.\n",
    "\n",
    "\n",
    "## 2. TODO: \n",
    "    Consider transferring mutate() method from Organism to Chromosome, while potentially keeping crossover() method as part of organism's namespace. Purpose is to encapsulate functionality as close as possible with the data/abstractions it will operate on\n",
    "\n",
    "\n",
    "## 3. To Consider:\n",
    "    How can I quantify the information coverage and computational complexity of a given set of chromosome options? \n",
    "\n",
    "        a. Start with the raw # of permutations of all chromosome options\n",
    "        b. Adjust by the expected coverage for each variant. E.g. How much of the hyperparameter space are we covering in our naive uniform grid search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1749,
     "status": "ok",
     "timestamp": 1602853523959,
     "user": {
      "displayName": "Aritra Roy Gosthipaty",
      "photoUrl": "",
      "userId": "14289131178028582626"
     },
     "user_tz": -330
    },
    "id": "sW1FkzdgLd8i"
   },
   "outputs": [],
   "source": [
    "options_phase0 = {\n",
    "    'a_filter_size': [(1,1), (3,3), (5,5), (7,7), (9,9)],\n",
    "    'a_include_BN': [True, False],\n",
    "    'a_output_channels': [8, 16, 32, 64, 128, 256, 512],\n",
    "    'activation_type': [ReLU, ELU, LeakyReLU],\n",
    "    'b_filter_size': [(1,1), (3,3), (5,5), (7,7), (9,9)],\n",
    "    'b_include_BN': [True, False],\n",
    "    'b_output_channels': [8, 16, 32, 64, 128, 256, 512],\n",
    "    'include_pool': [True, False],\n",
    "    'pool_type': [MaxPool2D, AveragePooling2D],\n",
    "    'include_skip': [True, False]\n",
    "}\n",
    "\n",
    "options = {\n",
    "    'include_layer': [True, False],\n",
    "    'a_filter_size': [(1,1), (3,3), (5,5), (7,7), (9,9)],\n",
    "    'a_include_BN': [True, False],\n",
    "    'a_output_channels': [8, 16, 32, 64, 128, 256, 512],\n",
    "    'b_filter_size': [(1,1), (3,3), (5,5), (7,7), (9,9)],\n",
    "    'b_include_BN': [True, False],\n",
    "    'b_output_channels': [8, 16, 32, 64, 128, 256, 512],\n",
    "    'include_pool': [True, False],\n",
    "    'pool_type': [MaxPool2D, AveragePooling2D],\n",
    "    'include_skip': [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "def random_hyper(phase):\n",
    "    if phase == 0:\n",
    "        return {\n",
    "        'a_filter_size': options_phase0['a_filter_size'][np.random.randint(len(options_phase0['a_filter_size']))],\n",
    "        'a_include_BN': options_phase0['a_include_BN'][np.random.randint(len(options_phase0['a_include_BN']))],\n",
    "        'a_output_channels': options_phase0['a_output_channels'][np.random.randint(len(options_phase0['a_output_channels']))],\n",
    "        'activation_type': options_phase0['activation_type'][np.random.randint(len(options_phase0['activation_type']))],\n",
    "        'b_filter_size': options_phase0['b_filter_size'][np.random.randint(len(options_phase0['b_filter_size']))],\n",
    "        'b_include_BN': options_phase0['b_include_BN'][np.random.randint(len(options_phase0['b_include_BN']))],\n",
    "        'b_output_channels': options_phase0['b_output_channels'][np.random.randint(len(options_phase0['b_output_channels']))],\n",
    "        'include_pool': options_phase0['include_pool'][np.random.randint(len(options_phase0['include_pool']))],\n",
    "        'pool_type': options_phase0['pool_type'][np.random.randint(len(options_phase0['pool_type']))],\n",
    "        'include_skip': options_phase0['include_skip'][np.random.randint(len(options_phase0['include_skip']))]\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "        'a_filter_size': options['a_filter_size'][np.random.randint(len(options['a_filter_size']))],\n",
    "        'a_include_BN': options['a_include_BN'][np.random.randint(len(options['a_include_BN']))],\n",
    "        'a_output_channels': options['a_output_channels'][np.random.randint(len(options['a_output_channels']))],\n",
    "        'b_filter_size': options['b_filter_size'][np.random.randint(len(options['b_filter_size']))],\n",
    "        'b_include_BN': options['b_include_BN'][np.random.randint(len(options['b_include_BN']))],\n",
    "        'b_output_channels': options['b_output_channels'][np.random.randint(len(options['b_output_channels']))],\n",
    "        'include_pool': options['include_pool'][np.random.randint(len(options['include_pool']))],\n",
    "        'pool_type': options['pool_type'][np.random.randint(len(options['pool_type']))],\n",
    "        'include_layer': options['include_layer'][np.random.randint(len(options['include_layer']))],\n",
    "        'include_skip': options['include_skip'][np.random.randint(len(options['include_skip']))]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1514,
     "status": "ok",
     "timestamp": 1602853528363,
     "user": {
      "displayName": "Aritra Roy Gosthipaty",
      "photoUrl": "",
      "userId": "14289131178028582626"
     },
     "user_tz": -330
    },
    "id": "6L1SwfFOotpO"
   },
   "outputs": [],
   "source": [
    "class Organism:\n",
    "    def __init__(self,\n",
    "                 data: Dict[str,tf.data.Dataset],\n",
    "                 config=None,\n",
    "                 chromosome={},\n",
    "                 phase=0,\n",
    "                 prevBestOrganism=None):\n",
    "        '''\n",
    "        config is a . accessible dict object containing model params that will stay constant during evolution\n",
    "        chromosome is a dictionary of genes\n",
    "        phase is the phase that the individual belongs to\n",
    "        prevBestOrganism is the best organism of the previous phase\n",
    "        \n",
    "        TODO:\n",
    "        \n",
    "        1. implement to_json and from_json methods for copies\n",
    "        2. Separate out step where organism is associated with a dataset\n",
    "        '''\n",
    "        self.data = data\n",
    "        self.train_data = data['train']\n",
    "        self.val_data = data['val']\n",
    "        self.test_data = data['test']\n",
    "        self.config = config\n",
    "        self.phase = phase\n",
    "        self.chromosome = chromosome\n",
    "        self.prevBestOrganism=prevBestOrganism\n",
    "        if phase != 0:\n",
    "            # In a later stage, the model is made by\n",
    "            # attaching new layers to the prev best model\n",
    "            self.last_model = prevBestOrganism.model\n",
    "    \n",
    "    @property\n",
    "    def config(self):\n",
    "        return self._config\n",
    "    \n",
    "    @config.setter\n",
    "    def config(self, config=None):\n",
    "        config = config or OmegaConf.create({})\n",
    "        config.input_shape = config.input_shape or (224,224,3)\n",
    "        config.output_size = config.output_size or 38\n",
    "        config.epochs_per_organism = config.epochs_per_organism or 5\n",
    "        self._config = config\n",
    "    \n",
    "    def build_model(self):\n",
    "        '''\n",
    "        This is the function to build the keras model\n",
    "        '''\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "        inputs = Input(shape=self.config.input_shape)\n",
    "        if self.phase != 0:\n",
    "            # Slice the prev best model # Use the model as a layer # Attach new layer to the sliced model\n",
    "            intermediate_model = Model(inputs=self.last_model.input,\n",
    "                                       outputs=self.last_model.layers[-3].output)\n",
    "            for layer in intermediate_model.layers:\n",
    "                # To make the iteration efficient\n",
    "                layer.trainable = False\n",
    "            inter_inputs = intermediate_model(inputs)\n",
    "            x = Conv2D(filters=self.chromosome['a_output_channels'],\n",
    "                       padding='same',\n",
    "                       kernel_size=self.chromosome['a_filter_size'],\n",
    "                       use_bias=self.chromosome['a_include_BN'])(inter_inputs)\n",
    "            # This is to ensure that we do not randomly chose anothere activation\n",
    "            self.chromosome['activation_type'] = self.prevBestOrganism.chromosome['activation_type']\n",
    "        else:\n",
    "            # For PHASE 0 only\n",
    "            # input layer\n",
    "            x = Conv2D(filters=self.chromosome['a_output_channels'],\n",
    "                       padding='same',\n",
    "                       kernel_size=self.chromosome['a_filter_size'],\n",
    "                       use_bias=self.chromosome['a_include_BN'])(inputs)\n",
    "            \n",
    "        if self.chromosome['a_include_BN']:\n",
    "            x = BatchNormalization()(x)\n",
    "        x = self.chromosome['activation_type']()(x)\n",
    "        if self.chromosome['include_pool']:\n",
    "            x = self.chromosome['pool_type'](strides=(1,1),\n",
    "                                             padding='same')(x)\n",
    "        if self.phase != 0 and self.chromosome['include_layer'] == False:\n",
    "            # Except for PHASE0, there is a choice for\n",
    "            # the number of layers that the model wants\n",
    "            if self.chromosome['include_skip']:\n",
    "                y = Conv2D(filters=self.chromosome['a_output_channels'],\n",
    "                           kernel_size=(1,1),\n",
    "                           padding='same')(inter_inputs)\n",
    "                x = Add()([y,x])\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "            x = Dense(self.output_shape, activation='softmax')(x)\n",
    "        else:\n",
    "            # PHASE0 or no skip\n",
    "            # in the tail\n",
    "            x = Conv2D(filters=self.chromosome['b_output_channels'],\n",
    "                       padding='same',\n",
    "                       kernel_size=self.chromosome['b_filter_size'],\n",
    "                       use_bias=self.chromosome['b_include_BN'])(x)\n",
    "            if self.chromosome['b_include_BN']:\n",
    "                x = BatchNormalization()(x)\n",
    "            x = self.chromosome['activation_type']()(x)\n",
    "            if self.chromosome['include_skip']:\n",
    "                y = Conv2D(filters=self.chromosome['b_output_channels'],\n",
    "                           padding='same',\n",
    "                           kernel_size=(1,1))(inputs)\n",
    "                x = Add()([y,x])\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "            x = Dense(self.config.output_size, activation='softmax')(x)\n",
    "        self.model = Model(inputs=[inputs], outputs=[x])\n",
    "        self.model.compile(optimizer='adam',\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "        \n",
    "    def fitnessFunction(self,\n",
    "                        train_data,\n",
    "                        val_data,\n",
    "                        generation_number):\n",
    "        '''\n",
    "        This function is used to calculate the\n",
    "        fitness of an individual.\n",
    "        '''\n",
    "        wandb.init(**get_wandb_credentials(phase=self.phase,\n",
    "                                           generation_number=generation_number))\n",
    "        \n",
    "        self.model.fit(train_data,\n",
    "                       epochs=self.config.epochs_per_organism,\n",
    "                       callbacks=[WandbCallback()],\n",
    "                       verbose=1)\n",
    "        _, self.fitness = self.model.evaluate(val_data,\n",
    "                                              verbose=1)\n",
    "    def crossover(self,\n",
    "                  partner,\n",
    "                  generation_number):\n",
    "        '''\n",
    "        This function helps in making children from two\n",
    "        parent individuals.\n",
    "        '''\n",
    "        child_chromosome = {}\n",
    "        endpoint = np.random.randint(low=0, high=len(self.chromosome))\n",
    "        for idx, key in enumerate(self.chromosome):\n",
    "            if idx <= endpoint:\n",
    "                child_chromosome[key] = self.chromosome[key]\n",
    "            else:\n",
    "                child_chromosome[key] = partner.chromosome[key]\n",
    "        child = Organism(chromosome=child_chromosome,\n",
    "                         data=self.data,\n",
    "                         config=self.config,\n",
    "                         phase=self.phase,\n",
    "                         prevBestOrganism=self.prevBestOrganism)\n",
    "        child.build_model()\n",
    "        child.fitnessFunction(self.train_data,\n",
    "                              self.val_data,\n",
    "                              generation_number=generation_number)\n",
    "        return child\n",
    "    \n",
    "    def mutation(self, generation_number):\n",
    "        '''\n",
    "        One of the gene is to be mutated.\n",
    "        '''\n",
    "        index = np.random.randint(0, len(self.chromosome))\n",
    "        key = list(self.chromosome.keys())[index]\n",
    "        if  self.phase != 0:\n",
    "            self.chromosome[key] = options[key][np.random.randint(len(options[key]))]\n",
    "        else:\n",
    "            self.chromosome[key] = options_phase0[key][np.random.randint(len(options_phase0[key]))]\n",
    "        self.build_model()\n",
    "        self.fitnessFunction(self.train_data,\n",
    "                             self.val_data,\n",
    "                             generation_number=generation_number)\n",
    "    \n",
    "    def show(self):\n",
    "        '''\n",
    "        Util function to show the individual's properties.\n",
    "        '''\n",
    "        pp.pprint(self.config)\n",
    "        pp.pprint(self.chromosome)\n",
    "        \n",
    "    \n",
    "def get_wandb_credentials(phase: int, generation_number: int):\n",
    "    return dict(entity=\"jrose\",\n",
    "                project=f\"vlga-plant_village\",\n",
    "                group='KAGp{}'.format(phase),\n",
    "                job_type='g{}'.format(generation_number))\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEVFOsxlc8ef"
   },
   "source": [
    "# Generation\n",
    "This is a class that hold generations of models.\n",
    "\n",
    "1. fitSurvivalRate - The amount of fit individuals we want in the next generation.\n",
    "2. unfitSurvivalProb - The probability of sending unfit individuals\n",
    "3. mutationRate - The mutation rate to change genes in an individual.\n",
    "4. phase - The phase that the generation belongs to.\n",
    "5. population_size - The amount of individuals that the generation consists of.\n",
    "6. prevBestOrganism - The best organism (individual) is the last phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1153,
     "status": "ok",
     "timestamp": 1602853528916,
     "user": {
      "displayName": "Aritra Roy Gosthipaty",
      "photoUrl": "",
      "userId": "14289131178028582626"
     },
     "user_tz": -330
    },
    "id": "cfa1Y5TNVYcY"
   },
   "outputs": [],
   "source": [
    "class Generation:\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 generation_config,\n",
    "                 organism_config,\n",
    "                 phase,\n",
    "                 prevBestOrganism):\n",
    "        self.data = data\n",
    "        self.config = generation_config\n",
    "        self.organism_config = organism_config\n",
    "        self.population = []\n",
    "        self.generation_number = 0\n",
    "        self.phase = phase\n",
    "        # creating the first population: GENERATION_0\n",
    "        # can be thought of as the setup function\n",
    "        self.prevBestOrganism = prevBestOrganism or None\n",
    "        self.initialize_population()\n",
    "        \n",
    "    @property\n",
    "    def config(self):\n",
    "        return self._config\n",
    "    \n",
    "    @config.setter\n",
    "    def config(self, config=None):\n",
    "        config = config or OmegaConf.create({})\n",
    "        config.population_size = config.population_size or 5\n",
    "        config.num_generations_per_phase = config.num_generations_per_phase or 3\n",
    "        config.fitSurvivalRate = config.fitSurvivalRate or 0.5\n",
    "        config.unfitSurvivalProb = config.unfitSurvivalProb or 0.2\n",
    "        config.mutationRate = config.mutationRate or 0.1\n",
    "        config.num_phases = config.num_phases or 5\n",
    "        \n",
    "        self._config = config\n",
    "        self.__dict__.update(config)\n",
    "        \n",
    "        \n",
    "    def initialize_population(self):\n",
    "        '''\n",
    "        1. Create self.population_size individual organisms from scratch by randomly sampling an initial set of hyperparameters (a chromosome)\n",
    "        2. As each is instantiated, build its model\n",
    "        3. Assess their fitness one-by-one\n",
    "        4. Sort models by relative fitness so we have a (potentially) new Best Organism (best model)\n",
    "        4. Increment generation number to 1\n",
    "        '''\n",
    "\n",
    "        for idx in range(self.population_size):\n",
    "            print(f'Creating, training then testing organism {idx} of generation {self.generation_number} and phase {self.phase}')\n",
    "            org = Organism(chromosome=random_hyper(self.phase),\n",
    "                           data=self.data,\n",
    "                           config=self.organism_config,\n",
    "                           phase=self.phase,\n",
    "                           prevBestOrganism=self.prevBestOrganism)\n",
    "            org.build_model()\n",
    "            org.fitnessFunction(org.data['train'],\n",
    "                                org.data['test'],\n",
    "                                generation_number=self.generation_number)\n",
    "            self.population.append(org)\n",
    "\n",
    "        # sorts the population according to fitness (high to low)\n",
    "        self.sortModel()\n",
    "        self.generation_number += 1\n",
    "\n",
    "    def sortModel(self):\n",
    "        '''\n",
    "        sort the models according to the \n",
    "        fitness in descending order.\n",
    "        '''\n",
    "        fitness = [ind.fitness for ind in self.population]\n",
    "        sort_index = np.argsort(fitness)[::-1]\n",
    "        self.population = [self.population[index] for index in sort_index]\n",
    "\n",
    "    def generate(self):\n",
    "        '''\n",
    "        Generate a new generation in the same phase\n",
    "        '''\n",
    "        number_of_fit = int(self.population_size * self.fitSurvivalRate)\n",
    "        new_pop = self.population[:number_of_fit]\n",
    "        for individual in self.population[number_of_fit:]:\n",
    "            if np.random.rand() <= self.unfitSurvivalProb:\n",
    "                new_pop.append(individual)\n",
    "        for index, individual in enumerate(new_pop):\n",
    "            if np.random.rand() <= self.mutationRate:\n",
    "                new_pop[index].mutation(generation_number=self.generation_number)\n",
    "        fitness = [ind.fitness for ind in new_pop]\n",
    "        children=[]\n",
    "        for idx in range(self.population_size-len(new_pop)):\n",
    "            parents = np.random.choice(new_pop, replace=False, size=(2,), p=softmax(fitness))\n",
    "            A=parents[0]\n",
    "            B=parents[1]\n",
    "            child=A.crossover(B, generation_number=self.generation_number)\n",
    "            children.append(child)\n",
    "        self.population = new_pop+children\n",
    "        self.sortModel()\n",
    "        self.generation_number+=1\n",
    "\n",
    "    def evaluate(self, last=False):\n",
    "        '''\n",
    "        Evaluate the generation\n",
    "        '''\n",
    "        fitness = [ind.fitness for ind in self.population]\n",
    "        \n",
    "        wandb.log({'population_size':len(fitness)}, commit=False)\n",
    "        wandb.log({'Best fitness': fitness[0]}, commit=False)\n",
    "        wandb.log({'Average fitness': sum(fitness)/len(fitness)})\n",
    "        \n",
    "        self.population[0].show()\n",
    "        if last:\n",
    "            BestOrganism = self.population[0]\n",
    "            model_path = f'best-model-phase_{self.phase}.png'\n",
    "            tf.keras.utils.plot_model(BestOrganism.model, to_file=model_path)\n",
    "            wandb.log({\"best_model\": [wandb.Image(model_path, caption=f\"Best Model phase_{self.phase}\")]})\n",
    "            log_high_loss_examples(BestOrganism.test_dataset,\n",
    "                                   BestOrganism.model, \n",
    "                                   k=32)\n",
    "            \n",
    "            return BestOrganism\n",
    "#             return self.population[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 37514,
     "status": "error",
     "timestamp": 1602853256806,
     "user": {
      "displayName": "Aritra Roy Gosthipaty",
      "photoUrl": "",
      "userId": "14289131178028582626"
     },
     "user_tz": -330
    },
    "id": "3BbVfUL3nZrh",
    "outputId": "db81b47f-a3be-4b2b-88e3-f0b1db702b2f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# population_size = 5\n",
    "# num_generations_per_phase = 3\n",
    "# fitSurvivalRate = 0.5\n",
    "# unfitSurvivalProb = 0.2\n",
    "# mutationRate = 0.1\n",
    "# num_phases = 5\n",
    "# prevBestOrganism = None\n",
    "prevBestOrganism = None\n",
    "\n",
    "for phase in range(generation_config.num_phases):\n",
    "    print(\"PHASE {}\".format(phase))\n",
    "    generation = Generation(data=data,\n",
    "                            generation_config=generation_config,\n",
    "                            organism_config=organism_config,\n",
    "                            phase=phase,\n",
    "                            prevBestOrganism=prevBestOrganism)\n",
    "#     while generation.generation_number < num_generations_per_phase:\n",
    "    generation.generate()\n",
    "    if generation.generation_number == generation.num_generations_per_phase:\n",
    "        # Last generation is the phase\n",
    "        # print('I AM THE BEST IN THE PHASE')\n",
    "        prevBestOrganism = generation.evaluate(last=True)\n",
    "#         model_path = f'best-model-phase_{phase}.png'\n",
    "#         tf.keras.utils.plot_model(prevBestOrganism.model, to_file=model_path)\n",
    "#         wandb.log({\"best_model\": [wandb.Image(model_path, caption=f\"Best Model phase_{phase}\")]})\n",
    "#         log_high_loss_examples(prevBestOrganism.test_dataset,\n",
    "#                                prevBestOrganism.model, \n",
    "#                                k=32)\n",
    "    else:\n",
    "        generation.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Using tfds.features.ClassLabel\n",
    "\n",
    "# feature_labels = tfds.features.ClassLabel(names=vocab)\n",
    "# data = ['Potato___healthy',\n",
    "#         'Potato___Late_blight',\n",
    "#         'Raspberry___healthy',\n",
    "#         'Soybean___healthy',\n",
    "#         'Squash___Powdery_mildew',\n",
    "#         'Strawberry___healthy',\n",
    "#         'Strawberry___Leaf_scorch',\n",
    "#         'Tomato___Bacterial_spot',\n",
    "#         'Tomato___Early_blight',\n",
    "#         'Tomato___healthy']\n",
    "\n",
    "# data += data[::-1]\n",
    "# print([feature_labels.str2int(label) for label in data])\n",
    "# data = train_data\n",
    "# data_enc = data.map(lambda x,y: (x, feature_labels.int2str(y)))\n",
    "\n",
    "### 2. Using StringLookup and CategoryEncoding Layers\n",
    "\n",
    "# layer = StringLookup(vocabulary=vocab, num_oov_indices=0, mask_token=None)\n",
    "# i_layer = StringLookup(vocabulary=layer.get_vocabulary(), invert=True)\n",
    "# int_data = layer(data)\n",
    "\n",
    "# print(len(layer.get_vocabulary()))\n",
    "# print(len(class_encoder.class_list))\n",
    "# print(set(layer.get_vocabulary())==set(class_encoder.class_list))\n",
    "\n",
    "# i_layer = StringLookup(vocabulary=layer.get_vocabulary(), invert=True)\n",
    "# int_data = layer(data)\n",
    "\n",
    "# print(layer(data))\n",
    "# print(i_layer(int_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from tensorflow.keras.layers.experimental.preprocessing import StringLookup, CategoryEncoding\n",
    "# # data = tf.constant([\"a\", \"b\", \"c\", \"b\", \"c\", \"a\"])\n",
    "# # # Use StringLookup to build an index of the feature values\n",
    "# # indexer = StringLookup()\n",
    "# # indexer.adapt(data)\n",
    "# # # Use CategoryEncoding to encode the integer indices to a one-hot vector\n",
    "# # encoder = CategoryEncoding(output_mode=\"binary\")\n",
    "# # encoder.adapt(indexer(data))\n",
    "# # # Convert new test data (which includes unknown feature values)\n",
    "# # test_data = tf.constant([\"a\", \"b\", \"c\", \"d\", \"e\", \"\"])\n",
    "# # encoded_data = encoder(indexer(test_data))\n",
    "# # print(encoded_data)\n",
    "\n",
    "# vocab = [\"a\", \"b\", \"c\", \"d\"]\n",
    "# data = tf.constant([[\"a\", \"c\", \"d\"], [\"d\", \"z\", \"b\"]])\n",
    "# layer = StringLookup(vocabulary=vocab)\n",
    "# i_layer = StringLookup(vocabulary=layer.get_vocabulary(), invert=True)\n",
    "# int_data = layer(data)\n",
    "\n",
    "# print(layer(data))\n",
    "# print(i_layer(int_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
