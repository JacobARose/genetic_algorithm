{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composable Tensorflow Models\n",
    "- Based on [Keras Idiomatic Programmer Framework](https://github.com/GoogleCloudPlatform/keras-idiomatic-programmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_path = latest_checkpoint\n",
    "# reload_status = resnet.load_pretrained_weights(weights_path)\n",
    "# reload_status = resnet.model.load_weights(weights_path+'/chkpt')\n",
    "# print(dir(reload_status))\n",
    "\n",
    "# from genetic_algorithm.utils.logging_utils import get_datetime_str, PredictionResults\n",
    "# import datetime\n",
    "# def get_datetime_str(datetime_obj=None):\n",
    "#     '''Helper function to get formatted date and time as a str. Defaults to current time if none is passed.\n",
    "    \n",
    "#     Example:\n",
    "#         >> get_datetime_str()\n",
    "#         'Thu Dec 10 04:10:31 2020'\n",
    "#     '''\n",
    "    \n",
    "#     if datetime_obj is None:\n",
    "#         datetime_obj = datetime.datetime.utcnow()\n",
    "        \n",
    "#     return datetime_obj.strftime('%c')\n",
    "\n",
    "# from typing import List, Any, Dict\n",
    "# from genetic_algorithm import stateful\n",
    "\n",
    "# class PredictionResults(stateful.Stateful):\n",
    "    \n",
    "#     name: str = 'predictions'\n",
    "#     y_prob: np.ndarray = None\n",
    "#     y_true: np.ndarray = None\n",
    "#     class_names: List[str] = None\n",
    "#     extra_metadata: Dict[str,Any] = None\n",
    "        \n",
    "#     def __init__(self, y_prob, y_true, class_names=None, name='predictions', **extra_metadata):\n",
    "#         self._assign_values(y_prob=y_prob, y_true=y_true, class_names=class_names, name=name, **extra_metadata)\n",
    "        \n",
    "#     def _assign_values(self, y_prob, y_true, class_names=None, name='predictions', **extra_metadata):\n",
    "#         self.y_prob = y_prob # (N,M)\n",
    "#         self.y_true = y_true # (N,M) one_hot encoded\n",
    "#         self.class_names = class_names #list of len == M\n",
    "#         self.name = name\n",
    "#         self.extra_metadata = extra_metadata\n",
    "#         self._enforce_schema()\n",
    "        \n",
    "#     def _enforce_schema(self):\n",
    "#         assert self.y_prob is not None\n",
    "#         assert self.y_true is not None\n",
    "#         assert isinstance(self.y_prob, np.ndarray)\n",
    "#         assert isinstance(self.y_true, np.ndarray)\n",
    "#         assert self.y_prob.ndim == self.y_true.ndim == 2\n",
    "#         assert self.y_prob.shape == self.y_true.shape\n",
    "#         assert self.y_prob.shape[0] == self.num_samples\n",
    "#         if self.class_names:\n",
    "#             assert len(self.class_names) == self.num_classes\n",
    "            \n",
    "#         if len(self.extra_metadata) > 0:\n",
    "#             assert isinstance(self.extra_metadata, dict)\n",
    "#             for key, item in self.extra_metadata.items():\n",
    "#                 if isinstance(item, np.integer):\n",
    "#                     self.extra_metadata[key] = int(item)\n",
    "#                 elif isinstance(item, np.floating):\n",
    "#                     self.extra_metadata[key] = float(item)\n",
    "#                 elif isinstance(item, np.ndarray):\n",
    "#                     self.extra_metadata[key] = item.tolist()\n",
    "\n",
    "#     def get_y_true(self, one_hot=True):\n",
    "#         if one_hot:\n",
    "#             return self.y_true\n",
    "#         return np.argmax(self.y_true, axis=1)\n",
    "        \n",
    "#     @property\n",
    "#     def y_pred(self):\n",
    "#         return np.argmax(self.y_prob, axis=1)\n",
    "    \n",
    "#     @property\n",
    "#     def y_true(self):\n",
    "#         return self._y_true\n",
    "    \n",
    "#     @y_true.setter\n",
    "#     def y_true(self, y_true_new):\n",
    "#         '''Keep default format of instance's y_true as one_hot encoding while in memory, convert to sparse int encoding for serialization to disk.\n",
    "        \n",
    "#         '''\n",
    "#         if y_true_new.ndim == 1:\n",
    "#             if not issubclass(type(y_true_new[0]), np.integer):\n",
    "#                 y_true_new = y_true_new.astype(np.uint8)\n",
    "#             y_true_new = tf.one_hot(y_true_new, depth=self.num_classes).numpy()\n",
    "#         self._y_true = y_true_new\n",
    "\n",
    "    \n",
    "#     @property\n",
    "#     def num_classes(self):\n",
    "#         return self.y_prob.shape[1]\n",
    "\n",
    "#     @property\n",
    "#     def num_samples(self):\n",
    "#         return self.y_prob.shape[0]\n",
    "    \n",
    "    \n",
    "#     def get_state(self):\n",
    "#         y_true = self.get_y_true(one_hot=False)\n",
    "#         y_prob = self.y_prob\n",
    "#         state = {'meta':{\n",
    "#                          'name':self.name,\n",
    "#                          'num_classes':self.num_classes,\n",
    "#                          'num_samples':self.num_samples,\n",
    "#                          **{k:v for k,v in self.extra_metadata.items()}\n",
    "#                  },\n",
    "#                  'data':{\n",
    "#                          'class_names':self.class_names,\n",
    "#                          'y_true':y_true.tolist(), #Store more memory efficient representation of y_true\n",
    "#                          'y_prob':y_prob.tolist()\n",
    "#                  }}\n",
    "#         assert np.allclose(y_true, np.asarray(state['data']['y_true']))\n",
    "#         assert np.allclose(y_prob, np.asarray(state['data']['y_prob']))\n",
    "        \n",
    "#         return state\n",
    "    \n",
    "#     def set_state(self, state):        \n",
    "#         for key in ['class_names','y_true','y_prob']:\n",
    "#             assert key in state['data'].keys()\n",
    "            \n",
    "#         y_prob = np.asarray(state['data']['y_prob'])\n",
    "#         y_true = np.asarray(state['data']['y_true'])\n",
    "#         class_names =  state['data']['class_names']\n",
    "#         name = state['meta']['name']\n",
    "#         extra_metadata = {k:v for k,v in state['meta'].items() if k not in ['name','num_classes','num_samples']}\n",
    "#         self._assign_values(y_prob=y_prob, y_true=y_true, class_names=class_names, name=name, **extra_metadata)\n",
    "        \n",
    "#         assert self.num_classes == state['meta']['num_classes']\n",
    "#         assert self.num_samples == state['meta']['num_samples']\n",
    "#         assert self.name == state['meta']['name']\n",
    "        \n",
    "#     def __repr__(self):\n",
    "#         return '\\n'.join([f'{k}:\\n\\t{v}' for k,v in self.get_state()['meta'].items()])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['TFDS_DATA_DIR'] = '/media/data/jacob/tensorflow_datasets'\n",
    "\n",
    "# from pyleaves.utils import set_tf_config\n",
    "# set_tf_config(num_gpus=1)\n",
    "# import tensorflow as tf\n",
    "# import tensorflow.keras.backend as K\n",
    "# from genetic_algorithm.models.zoo.resnet.resnet_v2_c import ResNetV2\n",
    "\n",
    "# def build_basic_resnet_v2(input_shape=(32, 32, 3), n_classes=10):\n",
    "#     ''' Example for constructing/training a ResNet V2 model on CIFAR-10\n",
    "#     '''\n",
    "#     # Example of constructing a mini-ResNet\n",
    "#     groups = [ { 'n_filters' : 64, 'n_blocks': 1 },\n",
    "#                { 'n_filters': 128, 'n_blocks': 2 },\n",
    "#                { 'n_filters': 256, 'n_blocks': 2 }]\n",
    "#     resnet = ResNetV2(groups, input_shape=input_shape, n_classes=n_classes)\n",
    "#     resnet.model.summary()\n",
    "#     return resnet\n",
    "\n",
    "# def count_model_params(model, verbose=True):\n",
    "#     param_counts = {'trainable_params':\n",
    "#                           np.sum([K.count_params(w) for w in model.trainable_weights]),\n",
    "#                     'non_trainable_params':\n",
    "#                           np.sum([K.count_params(w) for w in model.non_trainable_weights])\n",
    "#                    }\n",
    "#     param_counts['total_params'] = param_counts['trainable_params'] + param_counts['non_trainable_params']\n",
    "             \n",
    "#     if verbose:\n",
    "#         pp({k:f'{v:,}' for k,v in param_counts.items()})\n",
    "#     return param_counts\n",
    "\n",
    "# resnet = build_basic_resnet_v2()\n",
    "# shapes = [v.shape for v in resnet.model.trainable_variables]\n",
    "# shapes\n",
    "# dir(resnet.model)\n",
    "# print({k:v for k,v in vars(resnet.model).items() if 'trainable' in k})\n",
    "# from tensorflow.keras.utils.layer_utils import count_params\n",
    "# trainable_count = count_params(model.trainable_weights)\n",
    "# non_trainable_count = count_params(model.non_trainable_weights)\n",
    "\n",
    "\n",
    "\n",
    "# model = resnet.model\n",
    "# count_model_params(model, verbose=True)\n",
    "# tf.keras.utils.plot_model(model,\n",
    "#                           to_file=Path(model_dir,f'model.png'),\n",
    "#                           show_shapes=True,\n",
    "#                           show_layer_names=True,\n",
    "#                           expand_nested=True,\n",
    "#                           dpi=96)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def plot_vert_and_horizontal_model_graphs():\n",
    "#     for rankdir in ['TB','LR']:\n",
    "#         tf.keras.utils.plot_model(model,\n",
    "#                                   to_file=Path(model_dir,f'model_{rankdir}.png'),\n",
    "#                                   show_shapes=True,\n",
    "#                                   show_layer_names=True,\n",
    "#                                   rankdir=rankdir,\n",
    "#                                   expand_nested=True,\n",
    "#                                   dpi=96)\n",
    "\n",
    "# print(f'Total params: {total_count:,}')\n",
    "# print(f'Trainable params: {trainable_count:,}')\n",
    "# print(f'Non-trainable params: {non_trainable_count:,}')\n",
    "\n",
    "# resnet.model.trainable_variables\n",
    "\n",
    "# int(sum(np.prod(p) for p in standardized_weight_shapes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO: implement easy transfer protocol with 2 steps\n",
    "    1. remove head\n",
    "    2. add head\n",
    "- Multi-task model could simply involve repeating step 2 for each new task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir = '/media/data/jacob/GitHub/genetic_algorithm/tests/example_results/cifar10'\n",
    "# resnet.cifar10(save=save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO: Implement separate Datasets class to manage selectively fformatting the dataset upon loading.\n",
    "e.g.\n",
    "\n",
    "class PNASDataset:\n",
    "\n",
    "    def get_train_data(self, as_numpy=False):\n",
    "        if as_numpy:\n",
    "            x_train, y_train = next(iter(self.train_data.take(1)))\n",
    "            data = (x_train, y_train)\n",
    "        else:\n",
    "            data = self.train_data\n",
    "        return data\n",
    "    \n",
    "    def get_val_data(self, as_numpy=False):\n",
    "    \n",
    "    def get_test_data(self, as_numpy=False):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# plt.rcParams[\"figure.figsize\"] = [10, 6]\n",
    "# Set up with a higher resolution screen (useful on Mac)\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "import os\n",
    "os.environ['TFDS_DATA_DIR'] = '/media/data/jacob/tensorflow_datasets'\n",
    "\n",
    "from pyleaves.utils import set_tf_config\n",
    "set_tf_config(num_gpus=1)\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "from pprint import pprint as pp\n",
    "from pathlib import Path\n",
    "model_dir = '.'\n",
    "from sklearn.model_selection import train_test_split\n",
    "from boltons.funcutils import partial\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "from typing import Tuple, Optional\n",
    "from genetic_algorithm.utils.logging_utils import get_datetime_str, PredictionResults\n",
    "from genetic_algorithm.models.zoo.resnet.resnet_v2_c import ResNetV2\n",
    "from genetic_algorithm.models.zoo.training_c import test_Training_predict\n",
    "\n",
    "# resnet_cifar100 = ResNetV2(n_layers, input_shape=input_shape, n_classes=n_classes) \n",
    "# resnet_cifar100.cifar100(epochs=20, decay=('cosine', 0), save=save_dir)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "def count_model_params(model, verbose=True):\n",
    "    param_counts = {'trainable_params':\n",
    "                          np.sum([K.count_params(w) for w in model.trainable_weights]),\n",
    "                    'non_trainable_params':\n",
    "                          np.sum([K.count_params(w) for w in model.non_trainable_weights])\n",
    "                   }\n",
    "    param_counts['total_params'] = param_counts['trainable_params'] + param_counts['non_trainable_params']\n",
    "             \n",
    "    if verbose:\n",
    "        pp({k:f'{v:,}' for k,v in param_counts.items()})\n",
    "    return param_counts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_layers=50\n",
    "dataset_name = 'PNAS'\n",
    "\n",
    "if dataset_name=='plant_village':\n",
    "    num_classes = 38    \n",
    "else:\n",
    "    num_classes=19\n",
    "\n",
    "model_name = 'resnet_50_v2'\n",
    "target_size = (256,256,3)\n",
    "# target_size = (128,128,3)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet_ga = ResNetV2(n_layers=num_layers,\n",
    "#                   input_shape=target_size,\n",
    "#                   num_classes=num_classes,\n",
    "#                   include_top=True)\n",
    "\n",
    "# resnet_ga.pnas_init(target_size=target_size[:2], batch_size=32, threshold=100)\n",
    "\n",
    "\n",
    "# hyperparameters = {'weights':'imagenet'}\n",
    "# # model_name = 'resnet_50_v2_tf_imagenet'\n",
    "# resnet_tf_imagenet = ResNetV2.from_tf_pretrained(model_name=model_name,\n",
    "#                             input_shape=target_size, \n",
    "#                             include_top=False,\n",
    "#                             num_classes=num_classes,\n",
    "#                             **hyperparameters)\n",
    "\n",
    "\n",
    "# save_dir = f'/media/data/jacob/GitHub/genetic_algorithm/tests/example_results/plant_village/res{target_size[0]}'\n",
    "# resnet.plant_village_init(target_size=target_size[:2], batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# save_dir = f'/media/data/jacob/GitHub/genetic_algorithm/tests/example_results/PNAS/res{target_size[0]}'\n",
    "# resnet.pnas(target_size=target_size[:2], batch_size=batch_size, threshold=100, save=save_dir, allow_resume=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# weights_path = '/media/data/jacob/GitHub/genetic_algorithm/tests/example_results/plant_village/res128/train'\n",
    "# resnet.plant_village(target_size=target_size[:2], epochs=20, decay=('cosine', 0), save=save_dir, allow_resume=True)#, skip_full_run=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMAGENET PRETRAINED PNAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyperparameters = {'weights':'imagenet'}\n",
    "\n",
    "resnet_tf_imagenet = ResNetV2.from_tf_pretrained(model_name=model_name,\n",
    "                                                 input_shape=target_size, \n",
    "                                                 include_top=False,\n",
    "                                                 num_classes=num_classes,\n",
    "                                                 **hyperparameters)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_name=\"PNAS\"\n",
    "num_layers=50\n",
    "num_classes = 19\n",
    "target_size = (256,256,3)\n",
    "batch_size = 32\n",
    "threshold=100\n",
    "\n",
    "save_dir = f'/media/data/jacob/GitHub/genetic_algorithm/tests/example_results/PNAS/{resnet_tf_imagenet.model_name}_tf_imagenet/{dataset_name}__res{target_size[0]}_thresh-{threshold}/frozen_search'\n",
    "\n",
    "# resnet_tf_imagenet.pnas_init(target_size=target_size[:2], batch_size=32, threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resnet_tf_imagenet.fit_paleoai_dataset(dataset_name=dataset_name,\n",
    "                                       target_size=target_size[:2],\n",
    "                                       batch_size=batch_size,\n",
    "                                       threshold=threshold,\n",
    "                                       epochs=30,\n",
    "                                       initial_frozen_layers=(0,-4),\n",
    "                                       save=save_dir, \n",
    "                                       allow_resume=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Extant Leaves on Imagenet pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyleaves.utils import set_tf_config\n",
    "set_tf_config(num_gpus=1)\n",
    "\n",
    "from pprint import pprint as pp\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from genetic_algorithm.models.zoo.resnet.resnet_v2_c import ResNetV2\n",
    "\n",
    "from contrastive_learning.data.extant import NUM_CLASSES as num_classes, NUM_SAMPLES as num_samples\n",
    "\n",
    "hyperparameters = {'weights':'imagenet'}\n",
    "\n",
    "dataset_name='Extant'\n",
    "model_name = 'resnet_50_v2'\n",
    "target_size = (512,512,3)\n",
    "# val_split = 0.2\n",
    "batch_size = 16\n",
    "seed = 457\n",
    "\n",
    "initial_frozen_layers = (0,-17)\n",
    "\n",
    "resnet_tf_imagenet = ResNetV2.from_tf_pretrained(model_name=model_name,\n",
    "                                                 input_shape=target_size, \n",
    "                                                 include_top=False,\n",
    "                                                 num_classes=num_classes,\n",
    "                                                 **hyperparameters)\n",
    "\n",
    "\n",
    "save_dir = f'/media/data/jacob/GitHub/genetic_algorithm/tests/example_results/{dataset_name}_family_10/{resnet_tf_imagenet.model_name}_tf_imagenet/{dataset_name}__res{target_size[0]}/init_frozen_layers={initial_frozen_layers}'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "print(f'save_dir = {save_dir}')\n",
    "pp(['num_classes:', num_classes, 'num_samples', num_samples])\n",
    "\n",
    "resnet_tf_imagenet.fit_paleoai_dataset(dataset_name='Extant',\n",
    "                                       target_size=target_size[:2],\n",
    "                                       batch_size=batch_size,\n",
    "                                       num_epochs=20,\n",
    "                                       decay=('cosine', 0),\n",
    "                                       initial_frozen_layers=initial_frozen_layers,\n",
    "                                       save=save_dir,\n",
    "                                       allow_resume=True, \n",
    "                                       seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate - PredictionResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from genetic_algorithm.models.zoo.training_c import test_Training_predict\n",
    "import wandb\n",
    "\n",
    "resnet_tf_imagenet.set_wandb_env_vars(project='ResNet50_v2',\n",
    "                                      group=f'resnet50_v2-{dataset_name}-res{target_size[0]}')\n",
    "run = wandb.init(reinit=True, job_type='test_trained', tags=['test', 'trained'])\n",
    "\n",
    "test_Training_predict(weights_paths=save_dir,\n",
    "                      dataset_name=dataset_name,\n",
    "                      num_layers=num_layers,\n",
    "                      num_classes=num_classes,\n",
    "                      target_size=target_size,\n",
    "                      batch_size=batch_size,\n",
    "                      last_allowed='train',\n",
    "                      run=run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extant Leaves ResNet image family classification \n",
    "    -- 3-11-2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contrastive_learning.data.extant import NUM_CLASSES as num_classes\n",
    "from contrastive_learning.data.get_dataset import get_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "# dataset_name=\"PNAS\"\n",
    "# num_classes = 19\n",
    "\n",
    "dataset_name=\"Extant\"\n",
    "num_classes = 174\n",
    "\n",
    "#########################\n",
    "seed = 456\n",
    "num_layers=50\n",
    "size = 256\n",
    "num_channels = 3\n",
    "target_size = (size,size,num_channels)\n",
    "batch_size = 32\n",
    "threshold=100\n",
    "val_split = 0.2\n",
    "\n",
    "#########################\n",
    "train_data, val_data, test_data = get_dataset(dataset='extant_sup',\n",
    "                                              batch_size=batch_size,\n",
    "                                              val_split=val_split,\n",
    "                                              target_size=target_size,\n",
    "                                              seed=seed)\n",
    "#########################\n",
    "print(type(train_data))\n",
    "# train_data, val_data, test_data = get_supervised(batch_size, size, val_split=0.1, seed=seed)\n",
    "\n",
    "assert num_classes == train_data.element_spec[1].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "issubclass(type(train_data), tf.python.data.ops.dataset_ops.BatchDataset) # tf.data.Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = get_dataset(dataset='extant_unsup',\n",
    "                                              batch_size=batch_size,\n",
    "                                              val_split=val_split,\n",
    "                                              target_size=target_size,\n",
    "                                              seed=seed)\n",
    "#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data\n",
    "val_data\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=8\n",
    "plt.imshow(x[i,...])\n",
    "print(f'min = {np.min(x[i,...])}, max = {np.max(x[i,...])}, mean = {np.mean(x[i,...])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HE NORMAL INITIALIZED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_ga = ResNetV2(num_layers=num_layers,\n",
    "                     input_shape=target_size,\n",
    "                     num_classes=num_classes,\n",
    "                     include_top=True)\n",
    "\n",
    "resnet_ga.model\n",
    "\n",
    "resnet_ga.pnas_init(target_size=target_size[:2], batch_size=32, threshold=100)\n",
    "\n",
    "\n",
    "save_dir = f'/media/data/jacob/GitHub/genetic_algorithm/tests/example_results/PNAS/{resnet_ga.model_name}/res{target_size[0]}'\n",
    "resnet_ga.pnas(target_size=target_size[:2], batch_size=32, threshold=100, epochs=30, save=save_dir, search_frozen_layers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left off 6 AM 12/14/2020\n",
    "\n",
    "Need to pass data through the models to compare their shapes I guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[l.name for l in resnet_tf_imagenet.model.layers[-2:]]\n",
    "\n",
    "[l.name for l in resnet_ga.model.layers[-2:]]\n",
    "\n",
    "resnet_ga.summary()\n",
    "\n",
    "resnet_tf_imagenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from genetic_algorithm.utils.testing_uils import count_model_params\n",
    "\n",
    "resnet_ga_params = count_model_params(resnet_ga.model, verbose=True)\n",
    "\n",
    "resnet_tf_imagenet_params = count_model_params(resnet_tf_imagenet.model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reinitLayers(model):\n",
    "#     session = K.get_session()\n",
    "#     for layer in model.layers: \n",
    "#         if isinstance(layer, keras.engine.network.Network):\n",
    "#             reinitLayers(layer)\n",
    "#             continue\n",
    "#         print(\"LAYER::\", layer.name)\n",
    "#         for v in layer.__dict__:\n",
    "#             v_arg = getattr(layer,v)\n",
    "#             if hasattr(v_arg,'initializer'):\n",
    "#                 initializer_method = getattr(v_arg, 'initializer')\n",
    "#                 initializer_method.run(session=session)\n",
    "#                 print('reinitializing layer {}.{}'.format(layer.name, v))\n",
    "                \n",
    "                \n",
    "                \n",
    "def reset_weights(model):\n",
    "    '''\n",
    "    Recursively reset weights of tf.keras model layers by scanning for and calling the correct initializer.\n",
    "    '''\n",
    "    \n",
    "    for layer in model.layers: \n",
    "        if isinstance(layer, tf.keras.Model):\n",
    "            reset_weights(layer)\n",
    "            continue\n",
    "        for k, initializer in layer.__dict__.items():\n",
    "            if \"initializer\" not in k:\n",
    "                continue\n",
    "            # find the corresponding variable\n",
    "            var = getattr(layer, k.replace(\"_initializer\", \"\"))\n",
    "            if var is not None:\n",
    "                var.assign(initializer(var.shape, var.dtype))\n",
    "                print(f'Reset weights for variable {k} with initializer {type(initializer)}')\n",
    "        print('\\n')\n",
    "                      \n",
    "\n",
    "reset_weights(resnet_ga.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import genetic_algorithm as ga\n",
    "# from tensorflow.keras import layers\n",
    "# tf.keras.layers.Conv2D = ga.models.zoo.layers_c.Layers.Conv2D\n",
    "# layers.Conv2D = ga.models.zoo.layers_c.Layers.Conv2D\n",
    "\n",
    "# from tensorflow.python.keras.layers import convolutional\n",
    "# convolutional.Conv2D =  ga.models.zoo.layers_c.Layers.Conv2D\n",
    "\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "\n",
    "model_params = {'include_top':False,\n",
    "                'weights':'imagenet',\n",
    "                'input_shape':(224,224,3)}\n",
    "\n",
    "base_model = ResNet50V2(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnorm_layers = [l.name for l in base_model.layers]# if 'norm' in l.name]\n",
    "\n",
    "# bnorm_layers = [(i, l) for i,l in enumerate(base_model.layers) if l.name.endswith('_bn')]\n",
    "\n",
    "\n",
    "seed = 42\n",
    "initializer_names = ['glorot_normal',\n",
    "                'glorot_uniform',\n",
    "                'he_normal',\n",
    "                'he_uniform',\n",
    "                'lecun_normal',\n",
    "                'lecun_uniform',\n",
    "                'orthogonal',\n",
    "                'random_normal',\n",
    "                'random_uniform',\n",
    "                'truncated_normal',\n",
    "                'variance_scaling',\n",
    "                'zeros']\n",
    "\n",
    "def get_initializer_by_name(name: str, seed: int=None):\n",
    "    if name in tf.keras.initializers.__dict__:\n",
    "        try:\n",
    "            initializer = tf.keras.initializers.__dict__[name](seed=seed)\n",
    "            print(name, seed, '1', initializer)\n",
    "        except Exception as e:\n",
    "            initializer = tf.keras.initializers.__dict__[name]()\n",
    "            print(name, '2')\n",
    "        return initializer\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "new_initializers = {'kernel_initializer':'he_normal',\n",
    "                     'bias_initializer':'zeros'}\n",
    "\n",
    "layer_initializers = []\n",
    "# def reset_model_weights(model, new_initializers: Dict[str,str]=None, seed: int=None, verify: bool=False):\n",
    "\n",
    "layer_initializer_names = ['gamma_initializer',\n",
    "                           'beta_initializer',\n",
    "                           'moving_mean_initializer',\n",
    "                           'moving_variance_initializer',\n",
    "                           'kernel_initializer',\n",
    "                           'bias_initializer'][::-1]\n",
    "\n",
    "\n",
    "\n",
    "if True:\n",
    "    model = base_model\n",
    "    new_initializers = new_initializers or {}\n",
    "    for k,v in new_initializers.items():\n",
    "        new_initializers[k] = get_initializer_by_name(name=v, seed=seed)\n",
    "    \n",
    "    for ix, layer in enumerate(model.layers):\n",
    "        layer_init = {}\n",
    "        for k, layer_param in vars(layer).items():\n",
    "            if 'initializer' in k:\n",
    "                if k in new_initializers:\n",
    "                    layer_init.update({k:new_initializers[k]})\n",
    "                else:\n",
    "                    layer_init.update({k:layer_param})\n",
    "        if len(layer_init):\n",
    "            layer_initializers.append((i,layer.name, layer_init))\n",
    "            cfg = layer.get_config()\n",
    "            old_weights = layer.get_weights()\n",
    "            shapes = [w.shape for w in old_weights]\n",
    "            new_weights = []\n",
    "            for init_name in layer_initializer_names:\n",
    "                if init_name in cfg:\n",
    "                    i = len(new_weights) - 1\n",
    "                    new_weights.append(layer_init[init_name](shapes[i]))\n",
    "#                     setattr(model.layers[ix], init_name, layer_init[init_name])\n",
    "#                     assert getattr(model.layers[ix],k) == v\n",
    "            assert len(new_weights) == len(old_weights)\n",
    "            model.layers[ix].set_weights(new_weights)\n",
    "        \n",
    "            print([np.allclose(new, old) for new, old in zip(model.layers[ix].get_weights(), old_weights)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(old_weights)\n",
    "\n",
    "layer_init\n",
    "\n",
    "cfg\n",
    "\n",
    "new_weights=[]\n",
    "\n",
    "for init_name in layer_initializer_names:\n",
    "    if init_name in cfg:\n",
    "        i = len(new_weights) - 1\n",
    "        new_weights.append(layer_init[init_name](shapes[i]))\n",
    "#                     setattr(model.layers[ix], init_name, layer_init[init_name])\n",
    "#                     assert getattr(model.layers[ix],k) == v\n",
    "assert len(new_weights) == len(old_weights)\n",
    "# model.layers[ix].set_weights(new_weights)\n",
    "\n",
    "\n",
    "new_weights[0].shape\n",
    "\n",
    "\n",
    "old_weights[0].shape\n",
    "\n",
    "len(new_weights)\n",
    "\n",
    "i\n",
    "\n",
    "shapes\n",
    "\n",
    "%debug\n",
    "\n",
    "%debug\n",
    "\n",
    "layer_initializers\n",
    "\n",
    "[w.name for w in model.layers[5].weights]\n",
    "\n",
    "\n",
    "\n",
    "model.layers[5].get_weights()\n",
    "\n",
    "model.layers[5].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "#         if hasattr(model.layers[ix], 'kernel_initializer') and \\\n",
    "#            hasattr(model.layers[ix], 'bias_initializer'):\n",
    "\n",
    "#             if new_initializer is None:\n",
    "#                 weight_initializer = model.layers[ix].kernel_initializer\n",
    "#                 bias_initializer = model.layers[ix].bias_initializer\n",
    "#             else:                \n",
    "#                 weight_initializer = new_initializer\n",
    "#                 bias_initializer = new_initializer\n",
    "\n",
    "#             old_weights = model.layers[ix].get_weights()\n",
    "#             if len(old_weights)==2:\n",
    "#                 old_weights, old_biases = old_weights\n",
    "#                 new_weights = [weight_initializer(shape=old_weights.shape),\n",
    "#                                bias_initializer(shape=old_biases.shape)]\n",
    "#             else:\n",
    "#                 new_weights = [weight_initializer(shape=old_weights[0].shape)]\n",
    "\n",
    "#             model.layers[ix].set_weights(new_weights)\n",
    "            \n",
    "#             if verify:\n",
    "#                 assert not np.allclose(old_weights, model.layers[ix].get_weights())\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w.shape for w in base_model.layers[5].get_weights()]\n",
    "\n",
    "l = base_model.layers[5]#.get_config()\n",
    "\n",
    "layer_initializers[0][2].keys()\n",
    "\n",
    "layer_initializers[1][2].keys()\n",
    "\n",
    "getattr(l, 'beta_initializer')\n",
    "\n",
    "k = 'beta_initializer'\n",
    "setattr(l, k, layer_initializers[1][2][k])\n",
    "\n",
    "print(l, k, layer_initializers[1][2][k])\n",
    "\n",
    "[l.beta,l.gamma]\n",
    "\n",
    "l.get_weights()[:2]\n",
    "\n",
    "layer_initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# def reset_model_weights(model, new_initializer: str=None, seed: int=None, verify: bool=False):\n",
    "#     new_initializer = get_initializer_by_name(name=new_initializer, seed=seed)\n",
    "    \n",
    "#     for ix, layer in enumerate(model.layers):\n",
    "#         if hasattr(model.layers[ix], 'kernel_initializer') and \\\n",
    "#            hasattr(model.layers[ix], 'bias_initializer'):\n",
    "\n",
    "#             if new_initializer is None:\n",
    "#                 weight_initializer = model.layers[ix].kernel_initializer\n",
    "#                 bias_initializer = model.layers[ix].bias_initializer\n",
    "#             else:                \n",
    "#                 weight_initializer = new_initializer\n",
    "#                 bias_initializer = new_initializer\n",
    "\n",
    "#             old_weights = model.layers[ix].get_weights()\n",
    "#             if len(old_weights)==2:\n",
    "#                 old_weights, old_biases = old_weights\n",
    "#                 new_weights = [weight_initializer(shape=old_weights.shape),\n",
    "#                                bias_initializer(shape=old_biases.shape)]\n",
    "#             else:\n",
    "#                 new_weights = [weight_initializer(shape=old_weights[0].shape)]\n",
    "\n",
    "#             model.layers[ix].set_weights(new_weights)\n",
    "            \n",
    "#             if verify:\n",
    "#                 assert not np.allclose(old_weights, model.layers[ix].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_initializers = []\n",
    "for i,l in enumerate(base_model.layers):\n",
    "    layer_init = {k:param for k, param in vars(l).items() if 'initializer' in k}\n",
    "    if len(layer_init):\n",
    "        layer_initializers.append((i,l.name, layer_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%debug\n",
    "# bnorm_l = base_model.layers[5]\n",
    "# bnorm_l.non_trainable_weights\n",
    "# dir(base_model.layers[5])\n",
    "# old_weights = base_model.get_weights()\n",
    "reset_model_weights(resnet._model, new_initializer='he_normal', seed=34)#, verify=True)\n",
    "# new_weights = base_model.get_weights()\n",
    "# import pytest\n",
    "# for old, new in zip(old_weights, new_weights):\n",
    "#     with pytest.raises(AssertionError):\n",
    "#         np.testing.assert_array_almost_equal(old, new)\n",
    "#     print(old.shape, new.shape)\n",
    "# layers = [(len(l.get_weights()), type(l.get_weights()), hasattr(l, 'kernel_initializer')) for l in model.layers]\n",
    "# model.layers[5].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.non_trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.non_trainable_variables[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet.model\n",
    "\n",
    "initializable_params = []\n",
    "for ix, l in enumerate(model.layers):\n",
    "    \n",
    "    l_init_params = []\n",
    "    for param in dir(l):\n",
    "        if 'initializer' in param:\n",
    "            l_init_params.append(param)\n",
    "    if len(l_init_params):\n",
    "        initializable_params.append(l_init_params)\n",
    "        print(ix, l_init_params)\n",
    "        \n",
    "        \n",
    "print(f'{len(initializable_params)} initializable layers in model, out of {len(model.layers)} layers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.load_pretrained_weights('/media/data/jacob/GitHub/genetic_algorithm/tests/example_results/PNAS/res128/train')\n",
    "vars(resnet._load_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from paleoai_data.utils import db_utils\n",
    "\n",
    "# data = db_utils.load_full_db(version='v0.2')\n",
    "\n",
    "# data = data.assign(raw_path=data.apply(lambda x: x.raw_path.replace('data_cifs_lrs','data_cifs'), axis=1),\n",
    "#                     path=data.apply(lambda x: x.path.replace('data_cifs_lrs','data_cifs'), axis=1))\n",
    "\n",
    "# pnas_data = data[data.dataset=='PNAS']\n",
    "# sample = pnas_data.sample(n=1)\n",
    "\n",
    "# path = sample['path'].values[0]\n",
    "# class_name = sample['family'].values[0]\n",
    "\n",
    "# def process_path(file_path):\n",
    "#     img = tf.io.read_file(file_path)\n",
    "#     img = tf.image.decode_jpeg(img)\n",
    "#     return img\n",
    "\n",
    "# img = process_path(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = list(resnet.class_encoder.class_names.keys())\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = tf.expand_dims(img, 0) # Create a batch\n",
    "img_array = tf.image.grayscale_to_rgb(img_array)\n",
    "\n",
    "predictions = resnet.predict(img_array)\n",
    "# score = tf.nn.softmax(predictions[0])\n",
    "score = resnet.class_encoder.int2str(np.argmax(predictions,axis=1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\n",
    "#     \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "#     .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = resnet.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplots(1,1,figsize=(2,2))\n",
    "plt.gca().imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet.model\n",
    "\n",
    "variable_names = [v.name for v in model.weights]\n",
    "\n",
    "pp(variable_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(ckpt)\n",
    "\n",
    "\n",
    "(ckpt.save_path_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genetic_algorithm.utils.testing_uils import deep_getsizeof\n",
    "\n",
    "deep_getsizeof(resnet._load_status.__dict__,set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = f'/media/data/jacob/GitHub/genetic_algorithm/tests/example_results/plant_village/res{target_size[0]}'\n",
    "save_dir = f'/media/data/jacob/GitHub/genetic_algorithm/tests/example_results/plant_village/res224' #{target_size[0]*2}'\n",
    "\n",
    "latest_checkpoint = Checkpoint.find_latest_checkpoint(save_dir, last_allowed='pretext')\n",
    "\n",
    "weights_path = latest_checkpoint\n",
    "reload_status = resnet.load_pretrained_weights(weights_path)\n",
    "\n",
    "y_prob = resnet.model.predict(resnet.x_test)\n",
    "class_encoder = resnet.class_encoder\n",
    "y_true = resnet.y_test\n",
    "class_names = class_encoder.class_names\n",
    "prediction_results = PredictionResults(y_prob, y_true, class_names=class_names, name='test_predictions', dataset_name='plant_village', creation_date=get_datetime_str(), model_name = resnet.model_name, groups=resnet.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genetic_algorithm.utils.logging_utils import PredictionMetrics\n",
    "\n",
    "prediction_metrics = PredictionMetrics(prediction_results)\n",
    "\n",
    "prediction_metrics\n",
    "\n",
    "classification_report = prediction_metrics.classification_report('class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "print('Recall:')\n",
    "print(f\"micro: {prediction_metrics.recall('micro'):.2%}\",\n",
    "      f\"macro: {prediction_metrics.recall('macro'):.2%}\")\n",
    "\n",
    "print('Precision')\n",
    "print(f\"micro: {prediction_metrics.precision('micro'):.2%}\",\n",
    "      f\"macro: {prediction_metrics.precision('macro'):.2%}\")\n",
    "\n",
    "print('Accuracy')\n",
    "print(f\"micro: {prediction_metrics.accuracy('micro'):.2%}\",\n",
    "      f\"macro: {prediction_metrics.accuracy('macro'):.2%}\")\n",
    "\n",
    "print('F1-Score')\n",
    "print(f\"micro: {prediction_metrics.f1('micro'):.2%}\",\n",
    "      f\"macro: {prediction_metrics.f1('macro'):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification_report = []\n",
    "# agg_funcs = 'class'\n",
    "# classification_report = pd.DataFrame({'recall':prediction_metrics.recall(agg),\n",
    "#                               'precision':prediction_metrics.precision(agg),\n",
    "#                               'accuracy':prediction_metrics.accuracy(agg),\n",
    "#                               'f1-Score':prediction_metrics.f1(agg)},\n",
    "#                                     index=class_names)\n",
    "# # classification_report = pd.DataFrame.from_records(classification_report)\n",
    "# classification_report.index = class_names\n",
    "# classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.html import widgets\n",
    "\n",
    "def magnify():\n",
    "    return [dict(selector=\"th\",\n",
    "                 props=[(\"font-size\", \"16pt\")]),\n",
    "            dict(selector=\"td\",\n",
    "                 props=[('padding', \"0em 0em\")]),\n",
    "            dict(selector=\"th:hover\",\n",
    "                 props=[(\"font-size\", \"18pt\")]),\n",
    "            dict(selector=\"tr:hover td:hover\",\n",
    "                 props=[('max-width', '500px'),\n",
    "                        ('font-size', '16pt')])\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def display_classification_report(classification_report):\n",
    "    @widgets.interact\n",
    "    def f(h_neg=(0, 359, 1), h_pos=(0, 359), s=(0., 99.9), l=(0., 99.9)):\n",
    "        return df.style.background_gradient(\n",
    "            cmap=sns.palettes.diverging_palette(h_neg=h_neg, h_pos=h_pos, s=s, l=l,\n",
    "                                                as_cmap=True)\n",
    "        )\\\n",
    "        .set_precision(2)\\\n",
    "        .set_caption('Global summary metrics')\\\n",
    "        .set_table_styles(magnify())\n",
    "\n",
    "\n",
    "#################################################\n",
    "# def display_interactive_heatmap(df, cmap=None):\n",
    "#     cmap = cmap or sns.diverging_palette(5, 250, as_cmap=True)\n",
    "#     df.style.background_gradient(cmap, axis=1)\\\n",
    "#         .set_properties(**{'max-width': '100px', 'font-size': '3pt'})\\\n",
    "#         .set_caption(\"Hover to magnify\")\\\n",
    "#         .set_precision(2)\\\n",
    "#         .set_table_styles(magnify())\n",
    "\n",
    "display_classification_report(classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.style as style\n",
    "# styles_list = style.available\n",
    "%matplotlib inline\n",
    "from matplotlib import rcParams\n",
    "# rcParams['axes.titlepad'] = 20 \n",
    "# chosen_style = styles_list[0]\n",
    "# print('chosen_style = ',chosen_style)\n",
    "# style.use(chosen_style)\n",
    "style.use('seaborn-muted') #sets the size of the charts\n",
    "# style.use('ggplot')\n",
    "\n",
    "\n",
    "num_metrics = classification_report.shape[1]\n",
    "columns = classification_report.columns\n",
    "\n",
    "fig, axes = plt.subplots(num_metrics//2, num_metrics//2 ,figsize=(20*num_metrics//2, 10*num_metrics//2), sharex=True, sharey=True)\n",
    "fig.subplots_adjust(wspace=0.1, hspace=0.1, bottom=0.3, top=0.9)\n",
    "\n",
    "\n",
    "axes = axes.flat\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.barplot(data=classification_report.iloc[:,i].reset_index(), x='class', y=columns[i], ax=ax, palette='Wistia')\n",
    "\n",
    "    ax.set_ylim([0.0,1.0])\n",
    "    ax.set_ylabel(None)\n",
    "    ax.set_xlabel(None)\n",
    "    if i in [0, 1]:\n",
    "        ax.set_xticks([])\n",
    "        ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    else:\n",
    "        ax.tick_params(axis='x', rotation=80)\n",
    "        ax.set_xlabel('class', fontsize=18)\n",
    "    ax.set_title(columns[i].capitalize(), fontsize=20)\n",
    "    \n",
    "plt.suptitle('Per-Class Average Metrics', fontsize=35)\n",
    "\n",
    "    \n",
    "# classification_report.plot(kind='bar', width=0.9, ax=axes, subplots=True)\n",
    "# sns.set_style(styles_list[chosen_style])\n",
    "\n",
    "\n",
    "plt.tight_layout(pad=0.3, w_pad=0.15, h_pad=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def highlight_max(s):\n",
    "#     '''\n",
    "#     highlight the maximum in a Series yellow.\n",
    "#     '''\n",
    "#     is_max = s == s.max()\n",
    "#     return ['background-color: red' if v else 'background-color: grey' for v in is_max]\n",
    "\n",
    "# df = classification_report\n",
    "# df.style.background_gradient(cmap='Pastel1')#mako')\n",
    "# # highlight_max(color='green')\\\n",
    "# #    .highlight_min(color='red')\n",
    "# # df.style.bar(color='#d65f5f')\n",
    "# # cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "# # s = df.style.background_gradient(cmap=cm) \\\n",
    "# #             .set_precision(2) \\\n",
    "# #             .set_caption('Global summary metrics')\n",
    "\n",
    "df.plot.bar()\n",
    "\n",
    "np.random.seed(25)\n",
    "# bigdf = pd.DataFrame(generate_mock_confusion_matrix(num_classes=25, num_samples=10000, num_correct=1000, seed=25).to_array())\n",
    "# bigdf = bigdf.matrix\n",
    "\n",
    "\n",
    "# bigdf = pd.DataFrame(np.random.randn(20, 25)).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_metrics.get_values('micro')\n",
    "\n",
    "prediction_metrics.get_values('macro')\n",
    "\n",
    "prediction_metrics.get_values('class')\n",
    "\n",
    "prediction_metrics.get_values('sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genetic_algorithm.utils.data_utils import class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_onehot = prediction_results.get_y_pred(one_hot=True)\n",
    "\n",
    "y_true_onehot = prediction_results.get_y_true(one_hot=True)\n",
    "\n",
    "# tp = y_true_onehot*y_pred_onehot\n",
    "\n",
    "tp = np.sum(y_true_onehot*y_pred_onehot, axis=1)\n",
    "\n",
    "tp\n",
    "\n",
    "(tp==0).sum()\n",
    "\n",
    "a = np.where((y_true_onehot==0).astype(np.bool) & (y_pred_onehot==0).astype(np.bool))\n",
    "print(np.sum(not a))#, a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# True Positive (TP): we predict a label of 1 (positive), and the true label is 1.\n",
    "# tp = np.sum(np.logical_and(y_pred_onehot == 1, y_true_onehot == 1))\n",
    " \n",
    "# # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "# tn = np.sum(np.logical_and(y_pred_onehot == 0, y_true_onehot == 0))\n",
    " \n",
    "# # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "# fp = np.sum(np.logical_and(y_pred_onehot == 1, y_true_onehot == 0))\n",
    " \n",
    "# # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "# fn = np.sum(np.logical_and(y_pred_onehot == 0, y_true_onehot == 1))\n",
    "\n",
    "# ##############################################################\n",
    "# tp = (np.logical_and(y_pred_onehot == 1, y_true_onehot == 1))\n",
    " \n",
    "# # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "# tn = (np.logical_and(y_pred_onehot == 0, y_true_onehot == 0))\n",
    " \n",
    "# # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "# fp = (np.logical_and(y_pred_onehot == 1, y_true_onehot == 0))\n",
    " \n",
    "# # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "# fn = (np.logical_and(y_pred_onehot == 0, y_true_onehot == 1))\n",
    "##############################################################\n",
    "\n",
    "tp = np.sum(np.logical_and(y_pred_onehot == 1, y_true_onehot == 1), axis=1)\n",
    " \n",
    "# True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "tn = np.sum(np.logical_and(y_pred_onehot == 0, y_true_onehot == 0), axis=1)\n",
    " \n",
    "# False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "fp = np.sum(np.logical_and(y_pred_onehot == 1, y_true_onehot == 0), axis=1)\n",
    " \n",
    "# False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "fn = np.sum(np.logical_and(y_pred_onehot == 0, y_true_onehot == 1), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'tp: {tp}, tn: {tn}, fp: {fp}, fn: {fn}')\n",
    "\n",
    "\n",
    "print(f'tp: {tp.sum()}, tn: {tn.sum()}, fp: {fp.sum()}, fn: {fn.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp =np.logical_and(y_pred_onehot == 1, y_true_onehot == 1)\n",
    "\n",
    "print(np.sum(tp, axis=1).shape, np.sum(tp), np.sum(np.sum(tp, axis=1), axis=0), np.sum(tp, axis=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum([tp,tn,fp,fn]) == np.prod(y_pred_onehot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side-by-Side single-bar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_true = prediction_results.get_y_true(one_hot=False)\n",
    "y_pred = prediction_results.y_pred\n",
    "\n",
    "true_data_counts = class_counts(prediction_results.decode_names(y_true), as_dataframe=True)\n",
    "pred_data_counts = class_counts(prediction_results.decode_names(y_pred), as_dataframe=True)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(16,6))\n",
    "sns.barplot(x='label', y='label_count',data=true_data_counts, ax=ax[0])\n",
    "ax[0].tick_params(axis='x', rotation=90)#, horizontalalignment=\"center\")\n",
    "ax[0].set_title(f'True class counts')\n",
    "sns.barplot(x='label', y='label_count',data=pred_data_counts, ax=ax[1])\n",
    "ax[1].tick_params(axis='x', rotation=90)#, horizontalalignment=\"center\")\n",
    "# ax[1].set_xticks(rotation=30, horizontalalignment=\"center\")\n",
    "ax[1].set_title(f'Predicted class counts')\n",
    "plt.suptitle('True vs. Predicted Example Counts Per-Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstacked multi-bar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_true = prediction_results.get_y_true(one_hot=False)\n",
    "y_pred = prediction_results.y_pred\n",
    "class_names = prediction_results.class_names\n",
    "\n",
    "true_data_counts = class_counts(prediction_results.decode_names(y_true), as_dataframe=True).set_index('label')#.rename(columns={'label_count':'y_true'})\n",
    "pred_data_counts = class_counts(prediction_results.decode_names(y_pred), as_dataframe=True).set_index('label')#.rename(columns={'label_count':'y_pred'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([true_data_counts,pred_data_counts], axis=1)\n",
    "stacked_prediction_result_counts = pd.concat({'true':true_data_counts,'pred':pred_data_counts}, axis=1).swaplevel(0,1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked_prediction_result_counts.columns.levels\n",
    "# stacked_prediction_result_counts.swaplevel(0,1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.style as style\n",
    "styles_list = style.available\n",
    "\n",
    "dir(IPython.display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "chosen_style = styles_list[0]\n",
    "print('chosen_style = ',chosen_style)\n",
    "style.use(chosen_style)\n",
    "# sns.set_context('notebook')\n",
    "sns.set_context('talk')\n",
    "\n",
    "# sns.set_context('talk')  #\n",
    "fig, ax = plt.subplots(1,1,figsize=(20,10))\n",
    "\n",
    "stacked_prediction_result_counts.plot(kind='bar', cmap='Paired', width=0.9, ax=ax)\n",
    "# sns.set_style(styles_list[chosen_style])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def np_onehot(y: np.ndarray, depth: int):\n",
    "    '''Stand in replacement for tf.onehot().numpy()    \n",
    "    '''\n",
    "    return np.identity(depth)[y]\n",
    "\n",
    "def test_np_onehot():##y=None, depth=None):\n",
    "    y = np.array([0,4,2,1,3])\n",
    "    y_onehot_ground = np.array([[1., 0., 0., 0., 0.],\n",
    "                                [0., 0., 0., 0., 1.],\n",
    "                                [0., 0., 1., 0., 0.],\n",
    "                                [0., 1., 0., 0., 0.],\n",
    "                                [0., 0., 0., 1., 0.]])\n",
    "    \n",
    "    \n",
    "    y_one_hot = np_onehot(y, depth=5)\n",
    "    \n",
    "    assert np.all(y_one_hot == y_onehot_ground)\n",
    "    assert np.all(np.argmax(y_one_hot, axis=1) == y)\n",
    "\n",
    "\n",
    "\n",
    "# (np.argmax(a, axis=1) == prediction_results.y_pred).sum() == prediction_results.y_pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked_prediction_result_counts = pd.DataFrame({'true':true_data_counts,\n",
    "#                                               'pred':pred_data_counts}, index=class_names)\n",
    "\n",
    "# stacked_prediction_result_counts.plot(kind='bar', stacked=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stacked_prediction_result_counts.plot(kind='bar')\n",
    "\n",
    "# sns.barplot(x=stacked_prediction_result_counts.index,\n",
    "#             y=stacked_prediction_result_counts.columns,\n",
    "#             data=stacked_prediction_result_counts)\n",
    "\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(1,2, figsize=(16,6))\n",
    "# sns.barplot(x='label', y='label_count',data=true_data_counts, ax=ax[0])\n",
    "# ax[0].tick_params(axis='x', rotation=90)#, horizontalalignment=\"center\")\n",
    "# ax[0].set_title(f'True class counts')\n",
    "# sns.barplot(x='label', y='label_count',data=pred_data_counts, ax=ax[1])\n",
    "# ax[1].tick_params(axis='x', rotation=90)#, horizontalalignment=\"center\")\n",
    "# # ax[1].set_xticks(rotation=30, horizontalalignment=\"center\")\n",
    "# ax[1].set_title(f'Predicted class counts')\n",
    "# plt.suptitle('True vs. Predicted Example Counts Per-Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Tests\n",
    "- prediction saving + loading to + from disk\n",
    "- prediction logging + downloading to + from WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_results.save(fname=os.path.join(save_dir,'prediction_results.json'))\n",
    "prediction_results.reload(fname=os.path.join(save_dir,'prediction_results.json'))\n",
    "\n",
    "import wandb\n",
    "run = wandb.init()\n",
    "prediction_results.log_json_artifact(path=os.path.join(save_dir,'prediction_results.json'), run=run) #, artifact_type: str=None)\n",
    "\n",
    "artifact = run.use_artifact('jrose/genetic_algorithm-Notebooks/prediction_results.json:v0', type=\"<class '__main__.PredictionResults'>\")\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: COIL100 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir = '/media/data/jacob/GitHub/genetic_algorithm/tests/example_results/coil100'\n",
    "# resnet.coil100(epochs=20, decay=('cosine', 0), save=save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Procedures\n",
    "\n",
    "1. Pretext\n",
    "           zigsaw = int(sys.argv[_].split('=')[1])\n",
    "           jumpnet.pretext(zigsaw=zigsaw, lr=lr, batch_size=bs, save='cifar10')\n",
    "2. Pretrain/Warmup\n",
    "\n",
    "3. Train\n",
    "\n",
    "4. Evaluate\n",
    "\n",
    "\n",
    "## Model Macro and Micro Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import genetic_algorithm as ga\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=os.path.join(ga.__PACKAGE_RESOURCES__,'Macro-Architecture.jpg')))\n",
    "display(Image(filename=os.path.join(ga.__PACKAGE_RESOURCES__,'Micro-Architecture.jpg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ipyplot\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# from paleoai_data.utils import database_utils as db_utils\n",
    "\n",
    "# # load_full_db(db_path=local_db, version=None)\n",
    "# data = db_utils.load_full_db(db_path=None, version='v0.2')\n",
    "# num_samples = data.shape[0]\n",
    "# num_samples2display = 10\n",
    "# seed = 32\n",
    "# # rng = np.random.default_rng(seed)\n",
    "# # select_idx = rng.integers(low=0, high=num_samples, size=num_samples2display)\n",
    "# ipyplot.plot_images(images, max_images=10, img_width=512)\n",
    "\n",
    "# image_paths, labels = data.path.to_list(), data.family.to_list()\n",
    "\n",
    "# class_names = list(set(labels))\n",
    "# print(len(class_names))\n",
    "# class_names[:10]\n",
    "\n",
    "# # select = 20\n",
    "# image_paths = [image_paths[idx] for idx in select_idx]\n",
    "# labels      = [labels[idx] for idx in select_idx]\n",
    "\n",
    "# images = [Image.open(image) for image in image_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for sampling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyplot\n",
    "from PIL import Image\n",
    "from paleoai_data.utils import database_utils as db_utils\n",
    "# load_full_db(db_path=local_db, version=None)\n",
    "data = db_utils.load_full_db(db_path=None, version='v0.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "display.display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_data = data.assign(raw_path=data.apply(lambda x: x.raw_path.replace('data_cifs_lrs','data_cifs'), axis=1),\n",
    "                         path=data.apply(lambda x: x.path.replace('data_cifs_lrs','data_cifs'), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# checked = fixed_data[['original_path','raw_path','path']].applymap(lambda x: os.path.isfile(x))\n",
    "# checked.describe(include='all')\n",
    "# checked.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "# data.groupby(class_column).nth(0).set_index('id')\n",
    "\n",
    "def sample_nth_row_from_each_class(data: pd.DataFrame, n: int=0, class_column: str='family'):\n",
    "    return data.groupby(class_column) \\\n",
    "               .nth(0) \\\n",
    "               .reset_index() \\\n",
    "               .set_index('id')\n",
    "\n",
    "def sample_frac_rows_from_each_class(data: pd.DataFrame, class_column: str='family', frac: float=0.1, replace: bool=False, random_state: int=None):\n",
    "    return data.groupby(class_column) \\\n",
    "               .sample(frac=frac, replace=replace, random_state=random_state) \\\n",
    "               .reset_index() \\\n",
    "               .set_index('id')\n",
    "\n",
    "def load_data_from_df(data: pd.DataFrame, class_column='family', select_idx: List[int]=None):\n",
    "    \n",
    "    image_paths, labels = data.path.to_list(), data[class_column].to_list()\n",
    "    class_names = list(set(labels))\n",
    "    print(len(class_names))\n",
    "\n",
    "    # select = 20\n",
    "    if select_idx is not None:\n",
    "        image_paths = [image_paths[idx] for idx in select_idx]\n",
    "        labels      = [labels[idx] for idx in select_idx]\n",
    "\n",
    "    images = [Image.open(image) for image in image_paths]\n",
    "    \n",
    "    return images, labels, image_paths, class_names\n",
    "#######################################################\n",
    "class_column = 'family'\n",
    "queried_data = sample_nth_row_from_each_class(data=data,\n",
    "                                                 n=0,\n",
    "                                                 class_column=class_column)\n",
    "# queried_data = data.groupby(class_column).nth(0)\n",
    "\n",
    "# class_column = 'family'\n",
    "# queried_data = sample_frac_rows_from_each_class(data=data,\n",
    "#                                                  frac=0.1,\n",
    "#                                                  class_column=class_column)\n",
    "# queried_data = data.groupby(class_column).nth(0)\n",
    "\n",
    "images, labels, image_paths, class_names = load_data_from_df(data = queried_data, class_column=class_column)#, select_idx=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_list_ordered = [\"helmets\", \"boots\", \"insulated_jackets\", \"hardshell_jackets\", \"axes\", \"tents\"]\n",
    "labels_list_ordered = class_names[:10]\n",
    "ipyplot.plot_class_tabs(\n",
    "    images, labels, max_imgs_per_tab=5, tabs_order=labels_list_ordered\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipyplot.plot_class_representations(\n",
    "    images, labels)#, labels_order=labels_list_ordered\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "    \n",
    "- [x] (Completed 12/7/2020) Implemented basic CIFAR10 demo with multi-step training process\n",
    "- [ ] Implement the pre-text self-supervised learning method on CIFAR10\n",
    "- [x] (Completed 12/8/2020) Implemented basic CIFAR100 demo with multi-step training process\n",
    "- [x] (Completed 12/9/2020) Implemented basic PlantVillage demo with multi-step training process\n",
    "    - 99% test accuracy at 256 resolution\n",
    "- [ ] Implemented basic PNAS demo with multi-step training process\n",
    "- [ ] Implement separate loading and evaluation of saved model weights on held out test dataset\n",
    "- [ ] Implement fine-tuning and evaluation on trained model using a target dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features/Code Implementation\n",
    "\n",
    "- [x] (12/9/2020) PredictionResults schema class\n",
    "    - Easy get_state and set_state\n",
    "    - Easy Save and load from JSON into class instance\n",
    "    - log to wandb, query, download, and load into notebook for post-evaluation analysis\n",
    "- [ ] Replicate the PredictionResults class for hyperparameters\n",
    "    - Need a single Abstract Class to inherit from for different training stages/procedures/tasks\n",
    "        - warmup\n",
    "            - epochs\n",
    "            - start lr\n",
    "            - end lr\n",
    "            - batch_size\n",
    "            - loss\n",
    "        - tuning\n",
    "            - batch ranges\n",
    "- [x] (12/9/2020) Model save/load weights\n",
    "- [ ] WandB query fully saved model\n",
    "- [ ] Auto-checkpoint managing custom class for arbitrary model task\n",
    "        - (12/11/2020) Currently using patchwork custom checkpoint implementation, or\n",
    "        - class [NumpyWrapper(tf.train.experimental.PythonState)](https://www.tensorflow.org/api_docs/python/tf/train/experimental/PythonState)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## TODO\n",
    "\n",
    "- Taxonomize each of the training phases for CIFAR10 and CIFAR100\n",
    "- hyperparameters\n",
    "- scheduler: what does it change and when\n",
    "- evaluate: How are results reported\n",
    "- When is model saved and when is it loaded\n",
    "- Create UML diagram for all classes\n",
    "- Refactor Composable base class __repr__ method (or add a custom __dir__ method that pretty prints attributes and methods according to their proper modular definitions. e.g.\n",
    "- Refactor saving into OOP logger\n",
    "\n",
    "\n",
    "```\n",
    ">>> resnet.__dir__()\n",
    "'model'\n",
    "    'stem'\n",
    "    'learner'\n",
    "    'classifier'\n",
    "'data'\n",
    "    'splits'\n",
    "        'x_train'\n",
    "        'x_test'\n",
    "    'dataset loaders'\n",
    "        'cifar10'\n",
    "        'cifar100'\n",
    "        'coil100'\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## Stacked bar chart scratch notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'cat': [3, 5, 4, 3, 3, 23, 4],\n",
    "    'dog': [3, 23, 4, 3, 5, 4, 3],\n",
    "    'mouse': [3, 23, 4, 3, 5, 4, 3]\n",
    "})\n",
    "\n",
    "# Save the chart that's drawn\n",
    "# ax = df.plot(stacked=True, kind='barh', figsize=(10, 5))\n",
    "ax = df.plot(stacked=True, kind='bar', figsize=(5, 10))\n",
    "\n",
    "# .patches is everything inside of the chart, lines and\n",
    "# rectangles and circles and stuff. In this case we only\n",
    "# have rectangles!\n",
    "for rect in ax.patches:\n",
    "    # Find where everything is located\n",
    "    height = rect.get_height()\n",
    "    width = rect.get_width()\n",
    "    x = rect.get_x()\n",
    "    y = rect.get_y()\n",
    "    \n",
    "    # The width of the bar is also not pixels, it's the\n",
    "    # number of animals. So we can use it as the label!\n",
    "    label_text = width\n",
    "    \n",
    "    # ax.text(x, y, text)\n",
    "    label_x = x + width / 2\n",
    "    label_y = y + height / 2\n",
    "    ax.text(label_x, label_y, label_text, ha='center', va='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## Resize function scratch work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tf.config.run_functions_eagerly(True) \n",
    "\n",
    "\n",
    "# def resize_repeat(target_size: Tuple[int]):\n",
    "#     \"\"\"\n",
    "#     Adapted from Ivan's code located at\n",
    "#     https://github.com/serre-lab/tripletcyclegan/blob/master/data.py#L281\n",
    "\n",
    "#     Resize function that creates repetitions along the shortest image side to resize without aspect ratio distortion.\n",
    "\n",
    "#     Args:\n",
    "#       target_size ([type]): [description]\n",
    "\n",
    "#     Returns:\n",
    "#       [type]: [description]\n",
    "#     \"\"\"\n",
    "#     @tf.function\n",
    "#     def _map_fn(img):  # preprocessing\n",
    "#         img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "#         shape = tf.shape(img)\n",
    "#         tf.print(shape,  output_stream=sys.stdout)\n",
    "#         print(shape)\n",
    "#         print(img.shape)\n",
    "#         maxside = tf.math.maximum(shape[0],shape[1])\n",
    "#         minside = tf.math.minimum(shape[0],shape[1])\n",
    "#         new_img = img\n",
    "\n",
    "#         if tf.math.divide(maxside,minside) > 1.3:\n",
    "#             repeat = tf.math.floor(tf.math.divide(maxside,minside))  \n",
    "#             new_img = img\n",
    "#             if tf.math.equal(shape[1],minside):\n",
    "#                 for i in range(int(repeat)):\n",
    "#                     new_img = tf.concat((new_img, img), axis=1) \n",
    "#             if tf.math.equal(shape[0],minside):\n",
    "#                 for i in range(int(repeat)):\n",
    "#                     new_img = tf.concat((new_img, img), axis=0) \n",
    "#                 new_img = tf.image.rot90(new_img)\n",
    "#         else:\n",
    "#             new_img = img\n",
    "\n",
    "#         img = tf.image.resize(new_img, tuple(target_size))\n",
    "#         return img\n",
    "    \n",
    "#     return _map_fn\n",
    "\n",
    "# # import numpy as np\n",
    "# # from sklearn.model_selection import train_test_split\n",
    "# # # from tfrecord_utils.img_utils import resize_repeat\n",
    "# # from boltons.funcutils import partial\n",
    "\n",
    "# # import random\n",
    "# # import math\n",
    "# # import sys\n",
    "# # from typing import Tuple\n",
    "\n",
    "\n",
    "# # def get_parse_example_func(target_size=(224,224,3), num_classes=10):\n",
    "# #     resize = resize_repeat(target_size=tuple(target_size), training=False)\n",
    "# #     one_hot = partial(tf.one_hot, depth=num_classes)\n",
    "# #     def _parse_example(x, y):\n",
    "# #         x = tf.image.convert_image_dtype(x, tf.float32)\n",
    "# #         x = resize(x)\n",
    "# #         y = one_hot(y)\n",
    "# #         return x,y\n",
    "# #     return _parse_example\n",
    "# # parse_example = get_parse_example_func(target_size=target_size, num_classes=num_classes)\n",
    "# # resize = resize_repeat(target_size=tuple(target_size[:2]))\n",
    "\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(1)\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(1)\n",
    "\n",
    "# # # train_dataset.map(parse_example)\n",
    "# # resize = resize_repeat(target_size=tuple(target_size[:2]))\n",
    "\n",
    "# # print(tf.autograph.to_code(resize.python_function))\n",
    "\n",
    "# # resized = train_dataset.map(lambda x,y: (resize(x),y), num_parallel_calls=-1)\n",
    "\n",
    "# # train_dataset.element_spec[0].shape\n",
    "\n",
    "# # resize_img = partial(tf.image.resize, size=tuple(target_size[:2]))\n",
    "\n",
    "\n",
    "# # # def resize_imgs(size):\n",
    "# # #     def _resize(x):\n",
    "# # #         return tf.image.resize(x, size)\n",
    "# # #     return _resize\n",
    "\n",
    "\n",
    "# # # resize_img = resize_imgs(size=(128,128))\n",
    "\n",
    "\n",
    "# # resized = train_dataset.map(lambda x,y: (resize_img(x),y), num_parallel_calls=-1)\n",
    "\n",
    "# # x_train_resized = resize(x_train)\n",
    "\n",
    "# # def get_parse_example_func(target_size=(224,224,3), num_classes=10):\n",
    "# #     resize = resize_repeat(target_size=tuple(target_size), training=False)\n",
    "# #     one_hot = partial(tf.one_hot, depth=num_classes)\n",
    "# #     def _parse_example(x, y):\n",
    "# #         x = tf.image.convert_image_dtype(x, tf.float32)\n",
    "# #         x = resize(x)\n",
    "# #         y = one_hot(y)\n",
    "# #         return x,y\n",
    "# #     return _parse_example\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import plotly.express as px\n",
    "\n",
    "\n",
    "# import plotly.graph_objects as go\n",
    "# animals=['giraffes', 'orangutans', 'monkeys']\n",
    "\n",
    "# fig = go.Figure(data=[\n",
    "#     go.Bar(name='SF Zoo', x=animals, y=[20, 14, 23]),\n",
    "#     go.Bar(name='LA Zoo', x=animals, y=[12, 18, 29])\n",
    "# ])\n",
    "# # Change the bar mode\n",
    "# fig.update_layout(barmode='stack')#'group')\n",
    "# fig.show()\n",
    "\n",
    "# import plotly.graph_objects as go\n",
    "# animals=['giraffes', 'orangutans', 'monkeys']\n",
    "\n",
    "# fig = go.Figure(data=[\n",
    "#     go.Bar(name='SF Zoo', x=animals, y=[20, 14, 23]),\n",
    "#     go.Bar(name='LA Zoo', x=animals, y=[12, 18, 29])\n",
    "# ])\n",
    "# # Change the bar mode\n",
    "# fig.update_layout(barmode='group')\n",
    "# fig.show()\n",
    "\n",
    "# def plot_class_quantiles(data: pd.DataFrame, axis=None):\n",
    "#     # Calculate percentiles\n",
    "#     quant_5, quant_25, quant_50, quant_75, quant_95 = data.quantile(0.05), data.quantile(0.25), data.quantile(0.5), data.quantile(0.75), data.quantile(0.95)\n",
    "\n",
    "#     # [quantile, opacity, length]\n",
    "#     quants = [[quant_5, 0.6, 0.16],\n",
    "#               [quant_25, 0.8, 0.26],\n",
    "#               [quant_50, 1, 0.36],\n",
    "#               [quant_75, 0.8, 0.46],\n",
    "#               [quant_95, 0.6, 0.56]]\n",
    "#     if axis is None:\n",
    "#         fig, axis = plt.get_subplots(1,1)\n",
    "        \n",
    "#     data.value_counts().plot.hist(density=True, bins=200, axis=axis)#figsize=(2.5,2.5),)\n",
    "        \n",
    "#     # Plot the lines with a loop\n",
    "#     for i in quants:\n",
    "#         axis.axvline(i[0], alpha = i[1], ymax = i[2], linestyle = \":\")\n",
    "\n",
    "\n",
    "#     axis.text(quant_5-.1, 0.17, \"5th\", size = 10, alpha = 0.8)\n",
    "#     axis.text(quant_25-.13, 0.27, \"25th\", size = 11, alpha = 0.85)\n",
    "#     axis.text(quant_50-.13, 0.37, \"50th\", size = 12, alpha = 1)\n",
    "#     axis.text(quant_75-.13, 0.47, \"75th\", size = 11, alpha = 0.85)\n",
    "#     axis.text(quant_95-.25, 0.57, \"95th Percentile\", size = 10, alpha =.8)    \n",
    "\n",
    "# plot_class_quantiles(data=)\n",
    "\n",
    "# # Libraries\n",
    "# import matplotlib.pyplot as plt\n",
    " \n",
    "# # Make data: I have 3 groups and 7 subgroups\n",
    "# group_names=['groupA', 'groupB', 'groupC']\n",
    "# group_size=[12,11,30]\n",
    "# subgroup_names=['A.1', 'A.2', 'A.3', 'B.1', 'B.2', 'C.1', 'C.2', 'C.3', 'C.4', 'C.5']\n",
    "# subgroup_size=[4,3,5,6,5,10,5,5,4,6]\n",
    " \n",
    "# # Create colors\n",
    "# a, b, c=[plt.cm.Blues, plt.cm.Reds, plt.cm.Greens]\n",
    " \n",
    "# # First Ring (outside)\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.axis('equal')\n",
    "# mypie, _ = ax.pie(group_size, radius=1.3, labels=group_names, colors=[a(0.6), b(0.6), c(0.6)] )\n",
    "# plt.setp( mypie, width=0.3, edgecolor='white')\n",
    " \n",
    "# # Second Ring (Inside)\n",
    "# mypie2, _ = ax.pie(subgroup_size, radius=1.3-0.3, labels=subgroup_names, labeldistance=0.7, colors=[a(0.5), a(0.4), a(0.3), b(0.5), b(0.4), c(0.6), c(0.5), c(0.4), c(0.3), c(0.2)])\n",
    "# plt.setp( mypie2, width=0.4, edgecolor='white')\n",
    "# plt.margins(0,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet_tf_imagenet.model.layers[-6]\n",
    "\n",
    "# last_group = resnet_tf_imagenet.group(resnet_tf_imagenet.model.layers[-5].output,**{'n_filters':256,'n_blocks':3})\n",
    "\n",
    "# pooling = 'avg'\n",
    "# output = resnet_tf_imagenet.classifier(last_group, num_classes=num_classes, pooling=pooling)\n",
    "\n",
    "# resnet_tf_imagenet.inputs\n",
    "\n",
    "# resnet_tf_imagenet._model = tf.keras.models.Model(resnet_tf_imagenet.inputs, output)\n",
    "\n",
    "# resnet_tf_imagenet.summary()\n",
    "\n",
    "# from collections import defaultdict\n",
    "\n",
    "# class AttrDict(dict):\n",
    "#     __getattr__ = dict.__getitem__\n",
    "#     __setattr__ = dict.__setitem__\n",
    "\n",
    "# hparams = AttrDict(**{\"lr\":0.001, \"batch_size\":16, \"frozen_layers\":None, \"loss\":np.inf})\n",
    "\n",
    "# hparams.lr\n",
    "\n",
    "# hparams.update(lr=10,loss=0.0384)\n",
    "\n",
    "# hparams.update(frozen_layers=(0,3,4,5))\n",
    "\n",
    "# hparams\n",
    "\n",
    "# # hparams#.items()\n",
    "# # from collections import namedtuple\n",
    "# # HParams = AttrDict(\"lr\", \"batch_size\", \"frozen_layers\", \"loss\")\n",
    "# # hp = HParams(0.001, 32, (0,-1))\n",
    "# # hp.loss = 3.45\n",
    "# # hp.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if False:\n",
    "#     hyperparameters = {'weights':'imagenet'}\n",
    "\n",
    "#     frozen_layers_hp_range = [(0,-17), (0,-28), (0,-40), (0,-52), (0,-63)]\n",
    "\n",
    "#     for frozen_layers in frozen_layers_hp_range:\n",
    "#     # frozen_layers=(0,-4)\n",
    "#         K.clear_session()\n",
    "#         # model_name = 'resnet_50_v2_tf_imagenet'\n",
    "#         resnet_tf_imagenet = ResNetV2.from_tf_pretrained(model_name=model_name,\n",
    "#                                     input_shape=target_size, \n",
    "#                                     include_top=False,\n",
    "#                                     num_classes=num_classes,\n",
    "#                                     frozen_layers=frozen_layers,\n",
    "#                                     **hyperparameters)\n",
    "\n",
    "\n",
    "#         resnet_tf_imagenet.pnas_init(target_size=target_size[:2], batch_size=32, threshold=100)\n",
    "\n",
    "\n",
    "#         save_dir = f'/media/data/jacob/GitHub/genetic_algorithm/tests/example_results/PNAS/{resnet_tf_imagenet.model_name}_tf_imagenet/res{target_size[0]}/frozen_{frozen_layers}'\n",
    "\n",
    "#         resnet_tf_imagenet.pnas(target_size=target_size[:2], batch_size=32, threshold=100, epochs=30, save=save_dir, allow_resume=False) #True)\n",
    "\n",
    "#     resnet_tf_imagenet.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
