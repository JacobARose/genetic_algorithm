{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "setGPU: Setting GPU to: [4]\n",
      "Initial visible GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "visible GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Successfully set memory_growth=True and limited GPUs visible to tensorflow.\n",
      "\n",
      "Now using GPU(s):\n",
      "['/physical_device:GPU:0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/conda/jacob/envs/pyleaves2.3/lib/python3.7/site-packages/wandb/vendor/graphql-core-1.1/graphql/type/directives.py:55: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  assert isinstance(locations, collections.Iterable), 'Must provide locations for directive.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 26 16:14:09 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN X (Pascal)    Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 23%   21C    P8     7W / 250W |     10MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN X (Pascal)    Off  | 00000000:05:00.0 Off |                  N/A |\n",
      "| 23%   27C    P5    11W / 250W |     10MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  TITAN X (Pascal)    Off  | 00000000:08:00.0 Off |                  N/A |\n",
      "| 23%   24C    P8     8W / 250W |  10367MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  TITAN X (Pascal)    Off  | 00000000:09:00.0 Off |                  N/A |\n",
      "| 23%   29C    P8     8W / 250W |     10MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  TITAN X (Pascal)    Off  | 00000000:84:00.0 Off |                  N/A |\n",
      "| 23%   23C    P8     7W / 250W |     10MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  TITAN X (Pascal)    Off  | 00000000:85:00.0 Off |                  N/A |\n",
      "| 23%   23C    P8     8W / 250W |  10347MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  TITAN X (Pascal)    Off  | 00000000:88:00.0 Off |                  N/A |\n",
      "| 23%   21C    P8     8W / 250W |     10MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  TITAN X (Pascal)    Off  | 00000000:89:00.0 Off |                  N/A |\n",
      "| 23%   21C    P8     8W / 250W |     10MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    2     30717      C   /home/matt/newtorch-env/bin/python3        10357MiB |\n",
      "|    5     22761      C   ...ata/conda/minju/envs/selfsup/bin/python 10337MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# train_ds = data['train'].map(lambda x,y: (resize(x),y)).shuffle(1024).cache().batch(config.batch_size).prefetch(-1)\n",
    "def get_hardest_k_examples(test_dataset, model, k=32):\n",
    "    class_probs = model.predict(test_dataset)\n",
    "    predictions = np.argmax(class_probs, axis=1)\n",
    "    losses = tf.keras.losses.categorical_crossentropy(test_dataset.y, class_probs)\n",
    "    argsort_loss =  np.argsort(losses)\n",
    "\n",
    "    highest_k_losses = np.array(losses)[argsort_loss[-k:]]\n",
    "    hardest_k_examples = test_dataset.x[argsort_loss[-k:]]\n",
    "    true_labels = np.argmax(test_dataset.y[argsort_loss[-k:]], axis=1)\n",
    "\n",
    "    return highest_k_losses, hardest_k_examples, true_labels, predictions\n",
    "        \n",
    "def log_high_loss_examples(test_dataset, model, k=32):\n",
    "    print(f'logging k={k} hardest examples')\n",
    "    losses, hardest_k_examples, true_labels, predictions = get_hardest_k_examples(test_dataset, model, k=k)\n",
    "    wandb.log(\n",
    "        {\"high-loss-examples\":\n",
    "                            [wandb.Image(hard_example, caption = f'true:{label},\\npred:{pred}\\nloss={loss}')\n",
    "                             for hard_example, label, pred, loss in zip(hardest_k_examples, true_labels, predictions, losses)]\n",
    "        })\n",
    "\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pyleaves.utils import set_tf_config\n",
    "set_tf_config(num_gpus=1)\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "# wandb.login()\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPool2D, ReLU, ELU, LeakyReLU, Flatten, Dense, Add, AveragePooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup, CategoryEncoding\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(666)\n",
    "tf.random.set_seed(666)\n",
    "\n",
    "from typing import List, Tuple, Union, Dict\n",
    "import tensorflow_datasets as tfds\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from tfrecord_utils.img_utils import resize_repeat\n",
    "from boltons.funcutils import partial\n",
    "\n",
    "# import logging\n",
    "# logger = logging.getLogger('')\n",
    "\n",
    "LOG_DIR = '/media/data/jacob/GitHub/evolution_logs'\n",
    "import os\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "from paleoai_data.utils.logging_utils import get_logger\n",
    "logger = get_logger(logdir=LOG_DIR, filename='generation_evolution_logs.log', append=True)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/conda/jacob/envs/pyleaves2.3/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "TFDS_DATASETS = ['plant_village']\n",
    "\n",
    "class ClassLabelEncoder:\n",
    "    def __init__(self, ds_info: tfds.core.dataset_info.DatasetInfo):\n",
    "        self.info = ds_info\n",
    "        self.dataset_name = ds_info.full_name\n",
    "        self.num_samples = ds_info.splits['train'].num_examples\n",
    "        self.num_classes = ds_info.features['label'].num_classes\n",
    "        self.class_list = ds_info.features['label'].names\n",
    "        self._str2int = ds_info.features['label'].str2int\n",
    "        self._int2str = ds_info.features['label'].int2str\n",
    "        \n",
    "    def str2int(self, labels: Union[List[str],Tuple[str]]):\n",
    "        labels = _valid_eager_tensor(labels)\n",
    "        if not isinstance(labels, [list, tuple]):\n",
    "            assert isinstance(labels, str)\n",
    "            labels = [labels]\n",
    "        return [self._str2int(l) for l in labels]\n",
    "    \n",
    "    def int2str(self, labels: Union[List[int],Tuple[int]]):\n",
    "        labels = _valid_eager_tensor(labels)\n",
    "        if not isinstance(labels, [list, tuple]):\n",
    "            assert isinstance(labels, (int, np.int64))\n",
    "            labels = [labels]\n",
    "        return [self._int2str(l) for l in labels]\n",
    "    \n",
    "    def one_hot(self, label: tf.int64):\n",
    "        '''\n",
    "        One-Hot encode integer labels\n",
    "        Use tf.data.Dataset.map(lambda x,y: (x, encoder.one_hot(y))) and pass in individual labels already encoded in int64 format.\n",
    "        '''\n",
    "        return tf.one_hot(label, depth=self.num_classes)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'''Dataset Name: {self.dataset_name}\n",
    "        Num_samples: {self.num_samples}\n",
    "        Num_classes: {self.num_classes}'''\n",
    "    \n",
    "    def _valid_eager_tensor(self, tensor, strict=False):\n",
    "        '''\n",
    "        If tensor IS an EagerTensor, return tensor.numpy(). \n",
    "        if strict==True, and tensor IS NOT an EagerTensor, then raise AssertionError.\n",
    "        if strict==False, and tensor IS NOT an EagerTensor, then return tensor without modification \n",
    "        '''\n",
    "        try:\n",
    "            assert isinstance(labels, tf.python.framework.ops.EagerTensor)\n",
    "        except AssertionError:\n",
    "            if strict:\n",
    "                raise AssertionError(f'Strict EagerTensor requirement failed assertion test in ClassLabelEncoder._valid_eager_tensor method')\n",
    "        labels = labels.numpy()\n",
    "        return labels\n",
    "\n",
    "def load_plant_village_dataset(split=['train'],\n",
    "                               data_dir=None,\n",
    "                               batch_size=None):\n",
    "    \n",
    "    builder = tfds.builder('plant_village', data_dir=data_dir)\n",
    "    ds_info = builder.info\n",
    "    builder.download_and_prepare()\n",
    "\n",
    "    print(f'splits: {split}')\n",
    "    data = builder.as_dataset(split=list(split),\n",
    "                              shuffle_files=True,\n",
    "                              batch_size=batch_size,\n",
    "                              as_supervised=True\n",
    "                              )\n",
    "    \n",
    "    if not isinstance(data, (tuple, list)):\n",
    "        data = {'train':data}\n",
    "    elif len(data)==2:\n",
    "        data = {'train':data[0], 'val':data[1]}\n",
    "    elif len(data)==3:\n",
    "        data = {'train':data[0], 'val':data[1], 'test':data[2]}\n",
    "    \n",
    "    return data, builder\n",
    "\n",
    "def load_tfds_dataset(dataset_name='plant_village', \n",
    "                      split={'train':'train'},\n",
    "                      data_dir=None,\n",
    "                      batch_size=None):\n",
    "    '''\n",
    "    General interface function to properly route users to the correct function for loading their queried dataset from Tensorflow Datasets (TFDS) public data.\n",
    "    '''\n",
    "    assert dataset_name in TFDS_DATASETS\n",
    "    \n",
    "    print(f'Getting the TFDS dataset: {dataset_name}')\n",
    "    if dataset_name == 'plant_village':\n",
    "        return load_plant_village_dataset(split      =split,\n",
    "                                          data_dir   =data_dir,\n",
    "                                          batch_size =batch_size)\n",
    "    else:\n",
    "        raise Exception('Attempted to load dataset from TFDS that we have yet to build an adapter for. Consider building a minimal working prototype by using alternative datasets as a template.')\n",
    "    \n",
    "\n",
    "# def get_parse_example_func(target_size, class_encoder):\n",
    "#     resize = resize_repeat(target_size=target_size, training=False)\n",
    "#     def _parse_example(x, y):\n",
    "#         x = tf.image.convert_image_dtype(x, tf.float32)\n",
    "#         x = resize(x)\n",
    "#         y = class_encoder.one_hot(y)\n",
    "#         return x,y\n",
    "#     return _parse_example\n",
    "\n",
    "\n",
    "def get_parse_example_func(target_size, num_classes):\n",
    "    resize = resize_repeat(target_size=tuple(target_size), training=False)\n",
    "    one_hot = partial(tf.one_hot, depth=num_classes)\n",
    "    def _parse_example(x, y):\n",
    "        x = tf.image.convert_image_dtype(x, tf.float32)\n",
    "        x = resize(x)\n",
    "        y = one_hot(y)\n",
    "        return x,y\n",
    "    return _parse_example\n",
    "\n",
    "def preprocess_data(data: tf.data.Dataset, target_size=None, num_classes=None, batch_size=1): #class_encoder=None):\n",
    "    parse_example = get_parse_example_func(target_size=target_size, num_classes=num_classes) #class_encoder=class_encoder)\n",
    "    return data.map(lambda x,y: parse_example(x, y)) \\\n",
    "                .shuffle(1024) \\\n",
    "                .batch(batch_size) \\\n",
    "                .prefetch(-1)\n",
    "\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(data_config):\n",
    "\n",
    "    data, builder = load_tfds_dataset(dataset_name=data_config.load.dataset_name,\n",
    "                                      split=data_config.load.split,\n",
    "                                      data_dir=data_config.load.data_dir)\n",
    "\n",
    "    data_info     = builder.info\n",
    "    class_encoder = ClassLabelEncoder(data_info)\n",
    "    print(class_encoder)\n",
    "#     vocab = class_encoder.class_list\n",
    "    preprocess = partial(preprocess_data,\n",
    "                         batch_size=data_config.preprocess.batch_size,\n",
    "                         target_size=data_config.preprocess.target_size,\n",
    "                         num_classes=class_encoder.num_classes)\n",
    "\n",
    "    data['train'] = preprocess(data=data['train']) #, batch_size=config.batch_size)\n",
    "    data['val'] = preprocess(data=data['val']) #, batch_size=config.batch_size)\n",
    "    data['test'] = preprocess(data=data['test']) #, batch_size=config.batch_size)\n",
    "    \n",
    "    return data, class_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_and_log(dataset_name='plant_village', \n",
    "#                  data_dir    = '/media/data/jacob/tensorflow_datasets'):\n",
    "# dataset_name='plant_village'\n",
    "# data_dir = '/media/data/jacob/tensorflow_datasets'\n",
    "# if True:\n",
    "#     split = {\n",
    "#              'train': 'train[0%:60%]',\n",
    "#              'val': 'train[60%:70%]',\n",
    "#              'test': 'train[70%:100%]'\n",
    "#             }\n",
    "#     with wandb.init(project=\"artifacts-example\", job_type=\"load-data\") as run:\n",
    "#     run   = wandb.init(project=\"artifacts-example\", job_type=\"load-data\")        \n",
    "#     data, builder = load_tfds_dataset(dataset_name=dataset_name,\n",
    "#                                       split=split,\n",
    "#                                       data_dir=data_dir,\n",
    "#                                       batch_size=None)\n",
    "#     data_info     = builder.info\n",
    "#     raw_data      = wandb.Artifact(\n",
    "#                                     f\"{dataset_name}-raw\", type=\"dataset\",\n",
    "                                    \n",
    "#                                     description=\"Raw {plant_village} dataset, split into train/val/test\",\n",
    "#                                     metadata={\"source\": \"keras.datasets.mnist\",\n",
    "#                                               \"sizes\": [len(dataset.x) for dataset in datasets]})\n",
    "# split = {\n",
    "#          'train': 'train[0%:60%]',\n",
    "#          'val': 'train[60%:70%]',\n",
    "#          'test': 'train[70%:100%]'\n",
    "#         }    \n",
    "# data_dir = '/media/data/jacob/tensorflow_datasets'\n",
    "# data, ds_info = load_plant_village_dataset(split=split,\n",
    "#                                          data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and tracking label encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name='plant_village'\n",
    "data_dir = '/media/data/jacob/tensorflow_datasets'\n",
    "\n",
    "exp_config = OmegaConf.create({'seed':756, #237,\n",
    "                               'batch_size':16,\n",
    "                               'input_shape':(224,224,3),\n",
    "                               'output_size':38,\n",
    "                               'epochs_per_organism':3\n",
    "                              })\n",
    "\n",
    "data_config = OmegaConf.create({'load':{},'preprocess':{}})\n",
    "\n",
    "data_config['load'] = {'dataset_name':'plant_village',\n",
    "                       'split':['train[0%:60%]','train[60%:70%]','train[70%:100%]'],\n",
    "                       'data_dir':'/media/data/jacob/tensorflow_datasets'}\n",
    "\n",
    "data_config['preprocess'] = {'batch_size':exp_config.batch_size,\n",
    "                             'target_size':exp_config.input_shape[:2]}\n",
    "\n",
    "organism_config = OmegaConf.create({'input_shape':exp_config.input_shape,\n",
    "                                    'output_size':38,\n",
    "                                    'epochs_per_organism':5})\n",
    "generation_config = OmegaConf.create({\n",
    "                                      'population_size':5,\n",
    "                                      'num_generations_per_phase':3,\n",
    "                                      'fitSurvivalRate': 0.5,\n",
    "                                      'unfitSurvivalProb':0.2,\n",
    "                                      'mutationRate':0.1,\n",
    "                                      'num_phases':5\n",
    "                                    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = data_config.load.data_dir\n",
    "# split = data_config.load.split\n",
    "# batch_size = None\n",
    "\n",
    "# builder = tfds.builder('plant_village', data_dir=data_dir)\n",
    "# ds_info = builder.info\n",
    "# builder.download_and_prepare()\n",
    "\n",
    "# # print(f'splits: {split}')\n",
    "\n",
    "# # data = builder.as_dataset(as_supervised=True,\n",
    "# #                           split=split)\n",
    "\n",
    "# # # data = builder.as_dataset(split=split,\n",
    "# # #                           shuffle_files=True,\n",
    "# # #                           batch_size=batch_size,\n",
    "# # #                           as_supervised=True\n",
    "# # #                           )\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO Load dataset info from /media/data/jacob/tensorflow_datasets/plant_village/1.0.2\n",
      "INFO Reusing dataset plant_village (/media/data/jacob/tensorflow_datasets/plant_village/1.0.2)\n",
      "INFO Constructing tf.data.Dataset for split ['train[0%:60%]', 'train[60%:70%]', 'train[70%:100%]'], from /media/data/jacob/tensorflow_datasets/plant_village/1.0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the TFDS dataset: plant_village\n",
      "splits: ['train[0%:60%]', 'train[60%:70%]', 'train[70%:100%]']\n",
      "Dataset Name: plant_village/1.0.2\n",
      "        Num_samples: 54303\n",
      "        Num_classes: 38\n"
     ]
    }
   ],
   "source": [
    "data, class_encoder = load_and_preprocess_data(data_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZRopbXcoeJ6"
   },
   "source": [
    "# Organism\n",
    "An organism contains the following:\n",
    "\n",
    "1. phase - This denotes which phase does the organism belong to\n",
    "2. chromosome - A dictionary of genes (hyperparameters)\n",
    "3. model - The `tf.keras` model corresponding to the chromosome\n",
    "4. prevBestOrganism - The best organism in the previous **phase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1749,
     "status": "ok",
     "timestamp": 1602853523959,
     "user": {
      "displayName": "Aritra Roy Gosthipaty",
      "photoUrl": "",
      "userId": "14289131178028582626"
     },
     "user_tz": -330
    },
    "id": "sW1FkzdgLd8i"
   },
   "outputs": [],
   "source": [
    "options_phase0 = {\n",
    "    'a_filter_size': [(1,1), (3,3), (5,5), (7,7), (9,9)],\n",
    "    'a_include_BN': [True, False],\n",
    "    'a_output_channels': [8, 16, 32, 64, 128, 256, 512],\n",
    "    'activation_type': [ReLU, ELU, LeakyReLU],\n",
    "    'b_filter_size': [(1,1), (3,3), (5,5), (7,7), (9,9)],\n",
    "    'b_include_BN': [True, False],\n",
    "    'b_output_channels': [8, 16, 32, 64, 128, 256, 512],\n",
    "    'include_pool': [True, False],\n",
    "    'pool_type': [MaxPool2D, AveragePooling2D],\n",
    "    'include_skip': [True, False]\n",
    "}\n",
    "\n",
    "options = {\n",
    "    'include_layer': [True, False],\n",
    "    'a_filter_size': [(1,1), (3,3), (5,5), (7,7), (9,9)],\n",
    "    'a_include_BN': [True, False],\n",
    "    'a_output_channels': [8, 16, 32, 64, 128, 256, 512],\n",
    "    'b_filter_size': [(1,1), (3,3), (5,5), (7,7), (9,9)],\n",
    "    'b_include_BN': [True, False],\n",
    "    'b_output_channels': [8, 16, 32, 64, 128, 256, 512],\n",
    "    'include_pool': [True, False],\n",
    "    'pool_type': [MaxPool2D, AveragePooling2D],\n",
    "    'include_skip': [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "def random_hyper(phase):\n",
    "    if phase == 0:\n",
    "        return {\n",
    "        'a_filter_size': options_phase0['a_filter_size'][np.random.randint(len(options_phase0['a_filter_size']))],\n",
    "        'a_include_BN': options_phase0['a_include_BN'][np.random.randint(len(options_phase0['a_include_BN']))],\n",
    "        'a_output_channels': options_phase0['a_output_channels'][np.random.randint(len(options_phase0['a_output_channels']))],\n",
    "        'activation_type': options_phase0['activation_type'][np.random.randint(len(options_phase0['activation_type']))],\n",
    "        'b_filter_size': options_phase0['b_filter_size'][np.random.randint(len(options_phase0['b_filter_size']))],\n",
    "        'b_include_BN': options_phase0['b_include_BN'][np.random.randint(len(options_phase0['b_include_BN']))],\n",
    "        'b_output_channels': options_phase0['b_output_channels'][np.random.randint(len(options_phase0['b_output_channels']))],\n",
    "        'include_pool': options_phase0['include_pool'][np.random.randint(len(options_phase0['include_pool']))],\n",
    "        'pool_type': options_phase0['pool_type'][np.random.randint(len(options_phase0['pool_type']))],\n",
    "        'include_skip': options_phase0['include_skip'][np.random.randint(len(options_phase0['include_skip']))]\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "        'a_filter_size': options['a_filter_size'][np.random.randint(len(options['a_filter_size']))],\n",
    "        'a_include_BN': options['a_include_BN'][np.random.randint(len(options['a_include_BN']))],\n",
    "        'a_output_channels': options['a_output_channels'][np.random.randint(len(options['a_output_channels']))],\n",
    "        'b_filter_size': options['b_filter_size'][np.random.randint(len(options['b_filter_size']))],\n",
    "        'b_include_BN': options['b_include_BN'][np.random.randint(len(options['b_include_BN']))],\n",
    "        'b_output_channels': options['b_output_channels'][np.random.randint(len(options['b_output_channels']))],\n",
    "        'include_pool': options['include_pool'][np.random.randint(len(options['include_pool']))],\n",
    "        'pool_type': options['pool_type'][np.random.randint(len(options['pool_type']))],\n",
    "        'include_layer': options['include_layer'][np.random.randint(len(options['include_layer']))],\n",
    "        'include_skip': options['include_skip'][np.random.randint(len(options['include_skip']))]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1514,
     "status": "ok",
     "timestamp": 1602853528363,
     "user": {
      "displayName": "Aritra Roy Gosthipaty",
      "photoUrl": "",
      "userId": "14289131178028582626"
     },
     "user_tz": -330
    },
    "id": "6L1SwfFOotpO"
   },
   "outputs": [],
   "source": [
    "class Organism:\n",
    "    def __init__(self,\n",
    "                 data: Dict[str,tf.data.Dataset],\n",
    "                 config=None,\n",
    "                 chromosome={},\n",
    "                 phase=0,\n",
    "                 prevBestOrganism=None):\n",
    "        '''\n",
    "        config is a . accessible dict object containing model params that will stay constant during evolution\n",
    "        chromosome is a dictionary of genes\n",
    "        phase is the phase that the individual belongs to\n",
    "        prevBestOrganism is the best organism of the previous phase\n",
    "        \n",
    "        TODO:\n",
    "        \n",
    "        1. implement to_json and from_json methods for copies\n",
    "        2. Separate out step where organism is associated with a dataset\n",
    "        '''\n",
    "        self.data = data\n",
    "        self.train_data = data['train']\n",
    "        self.val_data = data['val']\n",
    "        self.test_data = data['test']\n",
    "        self.config = config\n",
    "        self.phase = phase\n",
    "        self.chromosome = chromosome\n",
    "        self.prevBestOrganism=prevBestOrganism\n",
    "        if phase != 0:\n",
    "            # In a later stage, the model is made by\n",
    "            # attaching new layers to the prev best model\n",
    "            self.last_model = prevBestOrganism.model\n",
    "    \n",
    "    @property\n",
    "    def config(self):\n",
    "        return self._config\n",
    "    \n",
    "    @config.setter\n",
    "    def config(self, config=None):\n",
    "        config = config or OmegaConf.create({})\n",
    "        config.input_shape = config.input_shape or (224,224,3)\n",
    "        config.output_size = config.output_size or 38\n",
    "        config.epochs_per_organism = config.epochs_per_organism or 5\n",
    "        self._config = config\n",
    "    \n",
    "    def build_model(self):\n",
    "        '''\n",
    "        This is the function to build the keras model\n",
    "        '''\n",
    "        K.clear_session()\n",
    "        inputs = Input(shape=self.config.input_shape)\n",
    "        if self.phase != 0:\n",
    "            # Slice the prev best model\n",
    "            # Use the model as a layer\n",
    "            # Attach new layer to the sliced model\n",
    "            intermediate_model = Model(inputs=self.last_model.input,\n",
    "                                       outputs=self.last_model.layers[-3].output)\n",
    "            for layer in intermediate_model.layers:\n",
    "                # To make the iteration efficient\n",
    "                layer.trainable = False\n",
    "            inter_inputs = intermediate_model(inputs)\n",
    "            x = Conv2D(filters=self.chromosome['a_output_channels'],\n",
    "                       padding='same',\n",
    "                       kernel_size=self.chromosome['a_filter_size'],\n",
    "                       use_bias=self.chromosome['a_include_BN'])(inter_inputs)\n",
    "            # This is to ensure that we do not randomly chose anothere activation\n",
    "            self.chromosome['activation_type'] = self.prevBestOrganism.chromosome['activation_type']\n",
    "        else:\n",
    "            # For PHASE 0 only\n",
    "            # input layer\n",
    "            x = Conv2D(filters=self.chromosome['a_output_channels'],\n",
    "                       padding='same',\n",
    "                       kernel_size=self.chromosome['a_filter_size'],\n",
    "                       use_bias=self.chromosome['a_include_BN'])(inputs)\n",
    "        if self.chromosome['a_include_BN']:\n",
    "            x = BatchNormalization()(x)\n",
    "        x = self.chromosome['activation_type']()(x)\n",
    "        if self.chromosome['include_pool']:\n",
    "            x = self.chromosome['pool_type'](strides=(1,1),\n",
    "                                             padding='same')(x)\n",
    "        if self.phase != 0 and self.chromosome['include_layer'] == False:\n",
    "            # Except for PHASE0, there is a choice for\n",
    "            # the number of layers that the model wants\n",
    "            if self.chromosome['include_skip']:\n",
    "                y = Conv2D(filters=self.chromosome['a_output_channels'],\n",
    "                           kernel_size=(1,1),\n",
    "                           padding='same')(inter_inputs)\n",
    "                x = Add()([y,x])\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "            x = Dense(self.output_shape, activation='softmax')(x)\n",
    "        else:\n",
    "            # PHASE0 or no skip\n",
    "            # in the tail\n",
    "            x = Conv2D(filters=self.chromosome['b_output_channels'],\n",
    "                       padding='same',\n",
    "                       kernel_size=self.chromosome['b_filter_size'],\n",
    "                       use_bias=self.chromosome['b_include_BN'])(x)\n",
    "            if self.chromosome['b_include_BN']:\n",
    "                x = BatchNormalization()(x)\n",
    "            x = self.chromosome['activation_type']()(x)\n",
    "            if self.chromosome['include_skip']:\n",
    "                y = Conv2D(filters=self.chromosome['b_output_channels'],\n",
    "                           padding='same',\n",
    "                           kernel_size=(1,1))(inputs)\n",
    "                x = Add()([y,x])\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "            x = Dense(self.config.output_size, activation='softmax')(x)\n",
    "        self.model = Model(inputs=[inputs], outputs=[x])\n",
    "        self.model.compile(optimizer='adam',\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "        \n",
    "    def fitnessFunction(self,\n",
    "                        train_data,\n",
    "                        val_data,\n",
    "                        generation_number):\n",
    "        '''\n",
    "        This function is used to calculate the\n",
    "        fitness of an individual.\n",
    "        '''\n",
    "        wandb.init(**get_wandb_credentials(phase=self.phase,\n",
    "                                           generation_number=generation_number))\n",
    "        \n",
    "        self.model.fit(train_data,\n",
    "                       epochs=self.config.epochs_per_organism,\n",
    "                       callbacks=[WandbCallback()],\n",
    "                       verbose=1)\n",
    "        _, self.fitness = self.model.evaluate(val_data,\n",
    "                                              verbose=1)\n",
    "    def crossover(self,\n",
    "                  partner,\n",
    "                  generation_number):\n",
    "        '''\n",
    "        This function helps in making children from two\n",
    "        parent individuals.\n",
    "        '''\n",
    "        child_chromosome = {}\n",
    "        endpoint = np.random.randint(low=0, high=len(self.chromosome))\n",
    "        for idx, key in enumerate(self.chromosome):\n",
    "            if idx <= endpoint:\n",
    "                child_chromosome[key] = self.chromosome[key]\n",
    "            else:\n",
    "                child_chromosome[key] = partner.chromosome[key]\n",
    "        child = Organism(chromosome=child_chromosome,\n",
    "                         data=self.data,\n",
    "                         config=self.config,\n",
    "                         phase=self.phase,\n",
    "                         prevBestOrganism=self.prevBestOrganism)\n",
    "        child.build_model()\n",
    "        child.fitnessFunction(self.train_data,\n",
    "                              self.val_data,\n",
    "                              generation_number=generation_number)\n",
    "        return child\n",
    "    \n",
    "    def mutation(self, generation_number):\n",
    "        '''\n",
    "        One of the gene is to be mutated.\n",
    "        '''\n",
    "        index = np.random.randint(0, len(self.chromosome))\n",
    "        key = list(self.chromosome.keys())[index]\n",
    "        if  self.phase != 0:\n",
    "            self.chromosome[key] = options[key][np.random.randint(len(options[key]))]\n",
    "        else:\n",
    "            self.chromosome[key] = options_phase0[key][np.random.randint(len(options_phase0[key]))]\n",
    "        self.build_model()\n",
    "        self.fitnessFunction(self.train_data,\n",
    "                             self.val_data,\n",
    "                             generation_number=generation_number)\n",
    "    \n",
    "    def show(self):\n",
    "        '''\n",
    "        Util function to show the individual's properties.\n",
    "        '''\n",
    "        pp.pprint(self.config)\n",
    "        pp.pprint(self.chromosome)\n",
    "        \n",
    "    \n",
    "def get_wandb_credentials(phase: int, generation_number: int):\n",
    "    return dict(entity=\"jrose\",\n",
    "                project=f\"vlga-plant_village\",\n",
    "                group='KAGp{}'.format(phase),\n",
    "                job_type='g{}'.format(generation_number))\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEVFOsxlc8ef"
   },
   "source": [
    "# Generation\n",
    "This is a class that hold generations of models.\n",
    "\n",
    "1. fitSurvivalRate - The amount of fit individuals we want in the next generation.\n",
    "2. unfitSurvivalProb - The probability of sending unfit individuals\n",
    "3. mutationRate - The mutation rate to change genes in an individual.\n",
    "4. phase - The phase that the generation belongs to.\n",
    "5. population_size - The amount of individuals that the generation consists of.\n",
    "6. prevBestOrganism - The best organism (individual) is the last phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1153,
     "status": "ok",
     "timestamp": 1602853528916,
     "user": {
      "displayName": "Aritra Roy Gosthipaty",
      "photoUrl": "",
      "userId": "14289131178028582626"
     },
     "user_tz": -330
    },
    "id": "cfa1Y5TNVYcY"
   },
   "outputs": [],
   "source": [
    "class Generation:\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 generation_config,\n",
    "                 organism_config,\n",
    "                 phase,\n",
    "                 prevBestOrganism):\n",
    "        self.data = data\n",
    "        self.config = generation_config\n",
    "        self.organism_config = organism_config\n",
    "        self.population = []\n",
    "        self.generation_number = 0\n",
    "        self.phase = phase\n",
    "        # creating the first population: GENERATION_0\n",
    "        # can be thought of as the setup function\n",
    "        self.prevBestOrganism = prevBestOrganism or None\n",
    "        self.initialize_population()\n",
    "        \n",
    "    @property\n",
    "    def config(self):\n",
    "        return self._config\n",
    "    \n",
    "    @config.setter\n",
    "    def config(self, config=None):\n",
    "        config = config or OmegaConf.create({})\n",
    "        config.population_size = config.population_size or 5\n",
    "        config.num_generations_per_phase = config.num_generations_per_phase or 3\n",
    "        config.fitSurvivalRate = config.fitSurvivalRate or 0.5\n",
    "        config.unfitSurvivalProb = config.unfitSurvivalProb or 0.2\n",
    "        config.mutationRate = config.mutationRate or 0.1\n",
    "        config.num_phases = config.num_phases or 5\n",
    "        \n",
    "        self._config = config\n",
    "        self.__dict__.update(config)\n",
    "        \n",
    "        \n",
    "    def initialize_population(self):\n",
    "        '''\n",
    "        1. Create self.population_size individual organisms from scratch by randomly sampling an initial set of hyperparameters (a chromosome)\n",
    "        2. As each is instantiated, build its model\n",
    "        3. Assess their fitness one-by-one\n",
    "        4. Sort models by relative fitness so we have a (potentially) new Best Organism (best model)\n",
    "        4. Increment generation number to 1\n",
    "        '''\n",
    "\n",
    "        for idx in range(self.population_size):\n",
    "            print(f'Creating, training then testing organism {idx} of generation {self.generation_number} and phase {self.phase}')\n",
    "            org = Organism(chromosome=random_hyper(self.phase),\n",
    "                           data=self.data,\n",
    "                           config=self.organism_config,\n",
    "                           phase=self.phase,\n",
    "                           prevBestOrganism=self.prevBestOrganism)\n",
    "            org.build_model()\n",
    "            org.fitnessFunction(org.data['train'],\n",
    "                                org.data['test'],\n",
    "                                generation_number=self.generation_number)\n",
    "            self.population.append(org)\n",
    "\n",
    "        # sorts the population according to fitness (high to low)\n",
    "        self.sortModel()\n",
    "        self.generation_number += 1\n",
    "\n",
    "    def sortModel(self):\n",
    "        '''\n",
    "        sort the models according to the \n",
    "        fitness in descending order.\n",
    "        '''\n",
    "        fitness = [ind.fitness for ind in self.population]\n",
    "        sort_index = np.argsort(fitness)[::-1]\n",
    "        self.population = [self.population[index] for index in sort_index]\n",
    "\n",
    "    def generate(self):\n",
    "        '''\n",
    "        Generate a new generation in the same phase\n",
    "        '''\n",
    "        number_of_fit = int(self.population_size * self.fitSurvivalRate)\n",
    "        new_pop = self.population[:number_of_fit]\n",
    "        for individual in self.population[number_of_fit:]:\n",
    "            if np.random.rand() <= self.unfitSurvivalProb:\n",
    "                new_pop.append(individual)\n",
    "        for index, individual in enumerate(new_pop):\n",
    "            if np.random.rand() <= self.mutationRate:\n",
    "                new_pop[index].mutation(generation_number=self.generation_number)\n",
    "        fitness = [ind.fitness for ind in new_pop]\n",
    "        children=[]\n",
    "        for idx in range(self.population_size-len(new_pop)):\n",
    "            parents = np.random.choice(new_pop, replace=False, size=(2,), p=softmax(fitness))\n",
    "            A=parents[0]\n",
    "            B=parents[1]\n",
    "            child=A.crossover(B, generation_number=self.generation_number)\n",
    "            children.append(child)\n",
    "        self.population = new_pop+children\n",
    "        self.sortModel()\n",
    "        self.generation_number+=1\n",
    "\n",
    "    def evaluate(self, last=False):\n",
    "        '''\n",
    "        Evaluate the generation\n",
    "        '''\n",
    "        fitness = [ind.fitness for ind in self.population]\n",
    "        \n",
    "        wandb.log({'population_size':len(fitness)}, commit=False)\n",
    "        wandb.log({'Best fitness': fitness[0]}, commit=False)\n",
    "        wandb.log({'Average fitness': sum(fitness)/len(fitness)})\n",
    "        \n",
    "        self.population[0].show()\n",
    "        if last:\n",
    "            BestOrganism = self.population[0]\n",
    "            model_path = f'best-model-phase_{self.phase}.png'\n",
    "            tf.keras.utils.plot_model(BestOrganism.model, to_file=model_path)\n",
    "            wandb.log({\"best_model\": [wandb.Image(model_path, caption=f\"Best Model phase_{self.phase}\")]})\n",
    "            log_high_loss_examples(BestOrganism.test_dataset,\n",
    "                                   BestOrganism.model, \n",
    "                                   k=32)\n",
    "            \n",
    "            return BestOrganism\n",
    "#             return self.population[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 37514,
     "status": "error",
     "timestamp": 1602853256806,
     "user": {
      "displayName": "Aritra Roy Gosthipaty",
      "photoUrl": "",
      "userId": "14289131178028582626"
     },
     "user_tz": -330
    },
    "id": "3BbVfUL3nZrh",
    "outputId": "db81b47f-a3be-4b2b-88e3-f0b1db702b2f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE 0\n",
      "Creating, training then testing organism 0 of generation 0 and phase 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "INFO setting login settings: {}\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjrose\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.7<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">glad-monkey-26</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/jrose/vlga-plant_village\" target=\"_blank\">https://wandb.ai/jrose/vlga-plant_village</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/jrose/vlga-plant_village/runs/11lxsm95\" target=\"_blank\">https://wandb.ai/jrose/vlga-plant_village/runs/11lxsm95</a><br/>\n",
       "                Run data is saved locally in <code>wandb/run-20201126_161414-11lxsm95</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2037/2037 [==============================] - 474s 233ms/step - loss: 1.8295 - accuracy: 0.4843\n",
      "Epoch 2/5\n",
      "2037/2037 [==============================] - 477s 234ms/step - loss: 1.3399 - accuracy: 0.6106\n",
      "Epoch 3/5\n",
      "2037/2037 [==============================] - 476s 234ms/step - loss: 1.1822 - accuracy: 0.6551\n",
      "Epoch 4/5\n",
      "2037/2037 [==============================] - 477s 234ms/step - loss: 1.0770 - accuracy: 0.6794\n",
      "Epoch 5/5\n",
      "2037/2037 [==============================] - 475s 233ms/step - loss: 0.9988 - accuracy: 0.7049\n",
      "1019/1019 [==============================] - 98s 96ms/step - loss: 2.7468 - accuracy: 0.3892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating, training then testing organism 1 of generation 0 and phase 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 20393<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.54MB of 0.54MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>wandb/run-20201126_161414-11lxsm95/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>wandb/run-20201126_161414-11lxsm95/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>4</td></tr><tr><td>loss</td><td>0.9988</td></tr><tr><td>accuracy</td><td>0.70487</td></tr><tr><td>_step</td><td>4</td></tr><tr><td>_runtime</td><td>2398</td></tr><tr><td>_timestamp</td><td>1606427652</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>loss</td><td>█▄▃▂▁</td></tr><tr><td>accuracy</td><td>▁▅▆▇█</td></tr><tr><td>_step</td><td>▁▃▅▆█</td></tr><tr><td>_runtime</td><td>▁▃▅▆█</td></tr><tr><td>_timestamp</td><td>▁▃▅▆█</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">glad-monkey-26</strong>: <a href=\"https://wandb.ai/jrose/vlga-plant_village/runs/11lxsm95\" target=\"_blank\">https://wandb.ai/jrose/vlga-plant_village/runs/11lxsm95</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.7<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">lyric-terrain-27</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/jrose/vlga-plant_village\" target=\"_blank\">https://wandb.ai/jrose/vlga-plant_village</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/jrose/vlga-plant_village/runs/33aagwlb\" target=\"_blank\">https://wandb.ai/jrose/vlga-plant_village/runs/33aagwlb</a><br/>\n",
       "                Run data is saved locally in <code>wandb/run-20201126_165553-33aagwlb</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2037/2037 [==============================] - 171s 84ms/step - loss: 2.4065 - accuracy: 0.3425\n",
      "Epoch 2/5\n",
      "2037/2037 [==============================] - 171s 84ms/step - loss: 1.8684 - accuracy: 0.4687\n",
      "Epoch 3/5\n",
      "2037/2037 [==============================] - 170s 84ms/step - loss: 1.6130 - accuracy: 0.5325\n",
      "Epoch 4/5\n",
      "2037/2037 [==============================] - 171s 84ms/step - loss: 1.4965 - accuracy: 0.5662\n",
      "Epoch 5/5\n",
      "2037/2037 [==============================] - 171s 84ms/step - loss: 1.4244 - accuracy: 0.5833\n",
      "1019/1019 [==============================] - 35s 35ms/step - loss: 1.6608 - accuracy: 0.51414s - loss: 1.6532 -  - ETA: 4s - loss: 1 - ETA: 3s - loss: 1.6519 - accuracy: 0.51 - ETA: 3s - loss: 1 - ETA: 0s - loss: 1.660 - ETA: 0s - loss: 1.6601 - accura\n",
      "Creating, training then testing organism 2 of generation 0 and phase 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10046<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.48MB of 0.48MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>wandb/run-20201126_165553-33aagwlb/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>wandb/run-20201126_165553-33aagwlb/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>4</td></tr><tr><td>loss</td><td>1.42437</td></tr><tr><td>accuracy</td><td>0.5833</td></tr><tr><td>_step</td><td>4</td></tr><tr><td>_runtime</td><td>868</td></tr><tr><td>_timestamp</td><td>1606428625</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>loss</td><td>█▄▂▂▁</td></tr><tr><td>accuracy</td><td>▁▅▇██</td></tr><tr><td>_step</td><td>▁▃▅▆█</td></tr><tr><td>_runtime</td><td>▁▃▅▆█</td></tr><tr><td>_timestamp</td><td>▁▃▅▆█</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">lyric-terrain-27</strong>: <a href=\"https://wandb.ai/jrose/vlga-plant_village/runs/33aagwlb\" target=\"_blank\">https://wandb.ai/jrose/vlga-plant_village/runs/33aagwlb</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.7<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">iconic-shadow-28</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/jrose/vlga-plant_village\" target=\"_blank\">https://wandb.ai/jrose/vlga-plant_village</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/jrose/vlga-plant_village/runs/3933021u\" target=\"_blank\">https://wandb.ai/jrose/vlga-plant_village/runs/3933021u</a><br/>\n",
       "                Run data is saved locally in <code>wandb/run-20201126_171104-3933021u</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   2/2037 [..............................] - ETA: 3:03 - loss: 4.2989 - accuracy: 0.0000e+00WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0649s vs `on_train_batch_end` time: 0.1139s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0649s vs `on_train_batch_end` time: 0.1139s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2037/2037 [==============================] - 372s 183ms/step - loss: 2.5507 - accuracy: 0.3168\n",
      "Epoch 2/5\n",
      "2037/2037 [==============================] - 372s 183ms/step - loss: 1.9321 - accuracy: 0.4671\n",
      "Epoch 3/5\n",
      "2037/2037 [==============================] - 371s 182ms/step - loss: 1.6390 - accuracy: 0.5402\n",
      "Epoch 4/5\n",
      "2037/2037 [==============================] - 371s 182ms/step - loss: 1.4845 - accuracy: 0.5743\n",
      "Epoch 5/5\n",
      "2037/2037 [==============================] - 371s 182ms/step - loss: 1.3755 - accuracy: 0.5995\n",
      "1019/1019 [==============================] - 65s 64ms/step - loss: 2.6333 - accuracy: 0.4218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating, training then testing organism 3 of generation 0 and phase 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 18277<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.49MB of 0.49MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>wandb/run-20201126_171104-3933021u/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>wandb/run-20201126_171104-3933021u/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>4</td></tr><tr><td>loss</td><td>1.3755</td></tr><tr><td>accuracy</td><td>0.5995</td></tr><tr><td>_step</td><td>4</td></tr><tr><td>_runtime</td><td>1876</td></tr><tr><td>_timestamp</td><td>1606430543</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>loss</td><td>█▄▃▂▁</td></tr><tr><td>accuracy</td><td>▁▅▇▇█</td></tr><tr><td>_step</td><td>▁▃▅▆█</td></tr><tr><td>_runtime</td><td>▁▃▄▆█</td></tr><tr><td>_timestamp</td><td>▁▃▄▆█</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">iconic-shadow-28</strong>: <a href=\"https://wandb.ai/jrose/vlga-plant_village/runs/3933021u\" target=\"_blank\">https://wandb.ai/jrose/vlga-plant_village/runs/3933021u</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.7<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">glad-galaxy-29</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/jrose/vlga-plant_village\" target=\"_blank\">https://wandb.ai/jrose/vlga-plant_village</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/jrose/vlga-plant_village/runs/8x3dao8a\" target=\"_blank\">https://wandb.ai/jrose/vlga-plant_village/runs/8x3dao8a</a><br/>\n",
       "                Run data is saved locally in <code>wandb/run-20201126_174331-8x3dao8a</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   2/2037 [..............................] - ETA: 5:58 - loss: 3.6966 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1063s vs `on_train_batch_end` time: 0.2454s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1063s vs `on_train_batch_end` time: 0.2454s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2037/2037 [==============================] - 720s 353ms/step - loss: 2.3514 - accuracy: 0.3517\n",
      "Epoch 2/5\n",
      "2037/2037 [==============================] - 717s 352ms/step - loss: 1.7811 - accuracy: 0.4915\n",
      "Epoch 3/5\n",
      "2037/2037 [==============================] - 719s 353ms/step - loss: 1.6064 - accuracy: 0.5347\n",
      "Epoch 4/5\n",
      "2037/2037 [==============================] - 720s 353ms/step - loss: 1.5462 - accuracy: 0.5526\n",
      "Epoch 5/5\n",
      "2037/2037 [==============================] - 720s 354ms/step - loss: 1.5074 - accuracy: 0.5627\n",
      "1019/1019 [==============================] - 104s 102ms/step - loss: 1.7117 - accuracy: 0.49331:12 - loss: 1.7055 - accuracy: 0. - E - ETA: 1:09 - loss: 1.6992 - ac - ETA: 36s - loss: 1.7012 - accuracy: 0.49 - E - E - ETA - ETA: 15s - loss: 1.6981 - a - ETA: 13s - loss: 1.6989 - accur - ETA: 12s - loss: 1.7008 - accuracy: 0. - ETA: 12s - loss: 1.7013 - accuracy: 0 - ETA: 1s - loss: 1.7108 - accuracy - ETA: 1s - loss: 1.7113 - accuracy - ETA: 1s - loss: 1.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating, training then testing organism 4 of generation 0 and phase 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2487<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.50MB of 0.50MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>wandb/run-20201126_174331-8x3dao8a/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>wandb/run-20201126_174331-8x3dao8a/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>4</td></tr><tr><td>loss</td><td>1.50738</td></tr><tr><td>accuracy</td><td>0.56267</td></tr><tr><td>_step</td><td>4</td></tr><tr><td>_runtime</td><td>3613</td></tr><tr><td>_timestamp</td><td>1606434228</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>loss</td><td>█▃▂▁▁</td></tr><tr><td>accuracy</td><td>▁▆▇██</td></tr><tr><td>_step</td><td>▁▃▅▆█</td></tr><tr><td>_runtime</td><td>▁▃▄▆█</td></tr><tr><td>_timestamp</td><td>▁▃▄▆█</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">glad-galaxy-29</strong>: <a href=\"https://wandb.ai/jrose/vlga-plant_village/runs/8x3dao8a\" target=\"_blank\">https://wandb.ai/jrose/vlga-plant_village/runs/8x3dao8a</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.7<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">effortless-pond-30</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/jrose/vlga-plant_village\" target=\"_blank\">https://wandb.ai/jrose/vlga-plant_village</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/jrose/vlga-plant_village/runs/3mlngzl3\" target=\"_blank\">https://wandb.ai/jrose/vlga-plant_village/runs/3mlngzl3</a><br/>\n",
       "                Run data is saved locally in <code>wandb/run-20201126_184535-3mlngzl3</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   2/2037 [..............................] - ETA: 1:06 - loss: 3.6473 - accuracy: 0.0312  WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0206s vs `on_train_batch_end` time: 0.0434s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0206s vs `on_train_batch_end` time: 0.0434s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2037/2037 [==============================] - 131s 64ms/step - loss: 2.8180 - accuracy: 0.2585\n",
      "Epoch 2/5\n",
      "2037/2037 [==============================] - 131s 64ms/step - loss: 2.2303 - accuracy: 0.3780\n",
      "Epoch 3/5\n",
      "2037/2037 [==============================] - 131s 64ms/step - loss: 1.9924 - accuracy: 0.4338\n",
      "Epoch 4/5\n",
      "2037/2037 [==============================] - 131s 64ms/step - loss: 1.8693 - accuracy: 0.4642\n",
      "Epoch 5/5\n",
      "2037/2037 [==============================] - 131s 64ms/step - loss: 1.7729 - accuracy: 0.4898\n",
      "1019/1019 [==============================] - 36s 36ms/step - loss: 1.8386 - accuracy: 0.4563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2269<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.51MB of 0.51MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>wandb/run-20201126_184535-3mlngzl3/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>wandb/run-20201126_184535-3mlngzl3/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>4</td></tr><tr><td>loss</td><td>1.77293</td></tr><tr><td>accuracy</td><td>0.48978</td></tr><tr><td>_step</td><td>4</td></tr><tr><td>_runtime</td><td>671</td></tr><tr><td>_timestamp</td><td>1606435010</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>loss</td><td>█▄▂▂▁</td></tr><tr><td>accuracy</td><td>▁▅▆▇█</td></tr><tr><td>_step</td><td>▁▃▅▆█</td></tr><tr><td>_runtime</td><td>▁▃▅▆█</td></tr><tr><td>_timestamp</td><td>▁▃▅▆█</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">effortless-pond-30</strong>: <a href=\"https://wandb.ai/jrose/vlga-plant_village/runs/3mlngzl3\" target=\"_blank\">https://wandb.ai/jrose/vlga-plant_village/runs/3mlngzl3</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.7<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">hardy-cloud-31</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/jrose/vlga-plant_village\" target=\"_blank\">https://wandb.ai/jrose/vlga-plant_village</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/jrose/vlga-plant_village/runs/rwt4uatb\" target=\"_blank\">https://wandb.ai/jrose/vlga-plant_village/runs/rwt4uatb</a><br/>\n",
       "                Run data is saved locally in <code>wandb/run-20201126_185730-rwt4uatb</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   2/2037 [..............................] - ETA: 5:54 - loss: 3.5229 - accuracy: 0.0312  WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1020s vs `on_train_batch_end` time: 0.2446s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1020s vs `on_train_batch_end` time: 0.2446s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2037/2037 [==============================] - 718s 352ms/step - loss: 2.3193 - accuracy: 0.3662\n",
      "Epoch 2/5\n",
      "1860/2037 [==========================>...] - ETA: 1:02 - loss: 1.8417 - accuracy: 0.4772"
     ]
    }
   ],
   "source": [
    "# population_size = 5\n",
    "# num_generations_per_phase = 3\n",
    "# fitSurvivalRate = 0.5\n",
    "# unfitSurvivalProb = 0.2\n",
    "# mutationRate = 0.1\n",
    "# num_phases = 5\n",
    "# prevBestOrganism = None\n",
    "prevBestOrganism = None\n",
    "\n",
    "for phase in range(generation_config.num_phases):\n",
    "    print(\"PHASE {}\".format(phase))\n",
    "    generation = Generation(data=data,\n",
    "                            generation_config=generation_config,\n",
    "                            organism_config=organism_config,\n",
    "                            phase=phase,\n",
    "                            prevBestOrganism=prevBestOrganism)\n",
    "#     while generation.generation_number < num_generations_per_phase:\n",
    "    generation.generate()\n",
    "    if generation.generation_number == generation.num_generations_per_phase:\n",
    "        # Last generation is the phase\n",
    "        # print('I AM THE BEST IN THE PHASE')\n",
    "        prevBestOrganism = generation.evaluate(last=True)\n",
    "#         model_path = f'best-model-phase_{phase}.png'\n",
    "#         tf.keras.utils.plot_model(prevBestOrganism.model, to_file=model_path)\n",
    "#         wandb.log({\"best_model\": [wandb.Image(model_path, caption=f\"Best Model phase_{phase}\")]})\n",
    "#         log_high_loss_examples(prevBestOrganism.test_dataset,\n",
    "#                                prevBestOrganism.model, \n",
    "#                                k=32)\n",
    "    else:\n",
    "        generation.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Using tfds.features.ClassLabel\n",
    "\n",
    "# feature_labels = tfds.features.ClassLabel(names=vocab)\n",
    "# data = ['Potato___healthy',\n",
    "#         'Potato___Late_blight',\n",
    "#         'Raspberry___healthy',\n",
    "#         'Soybean___healthy',\n",
    "#         'Squash___Powdery_mildew',\n",
    "#         'Strawberry___healthy',\n",
    "#         'Strawberry___Leaf_scorch',\n",
    "#         'Tomato___Bacterial_spot',\n",
    "#         'Tomato___Early_blight',\n",
    "#         'Tomato___healthy']\n",
    "\n",
    "# data += data[::-1]\n",
    "# print([feature_labels.str2int(label) for label in data])\n",
    "# data = train_data\n",
    "# data_enc = data.map(lambda x,y: (x, feature_labels.int2str(y)))\n",
    "\n",
    "### 2. Using StringLookup and CategoryEncoding Layers\n",
    "\n",
    "# layer = StringLookup(vocabulary=vocab, num_oov_indices=0, mask_token=None)\n",
    "# i_layer = StringLookup(vocabulary=layer.get_vocabulary(), invert=True)\n",
    "# int_data = layer(data)\n",
    "\n",
    "# print(len(layer.get_vocabulary()))\n",
    "# print(len(class_encoder.class_list))\n",
    "# print(set(layer.get_vocabulary())==set(class_encoder.class_list))\n",
    "\n",
    "# i_layer = StringLookup(vocabulary=layer.get_vocabulary(), invert=True)\n",
    "# int_data = layer(data)\n",
    "\n",
    "# print(layer(data))\n",
    "# print(i_layer(int_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from tensorflow.keras.layers.experimental.preprocessing import StringLookup, CategoryEncoding\n",
    "# # data = tf.constant([\"a\", \"b\", \"c\", \"b\", \"c\", \"a\"])\n",
    "# # # Use StringLookup to build an index of the feature values\n",
    "# # indexer = StringLookup()\n",
    "# # indexer.adapt(data)\n",
    "# # # Use CategoryEncoding to encode the integer indices to a one-hot vector\n",
    "# # encoder = CategoryEncoding(output_mode=\"binary\")\n",
    "# # encoder.adapt(indexer(data))\n",
    "# # # Convert new test data (which includes unknown feature values)\n",
    "# # test_data = tf.constant([\"a\", \"b\", \"c\", \"d\", \"e\", \"\"])\n",
    "# # encoded_data = encoder(indexer(test_data))\n",
    "# # print(encoded_data)\n",
    "\n",
    "# vocab = [\"a\", \"b\", \"c\", \"d\"]\n",
    "# data = tf.constant([[\"a\", \"c\", \"d\"], [\"d\", \"z\", \"b\"]])\n",
    "# layer = StringLookup(vocabulary=vocab)\n",
    "# i_layer = StringLookup(vocabulary=layer.get_vocabulary(), invert=True)\n",
    "# int_data = layer(data)\n",
    "\n",
    "# print(layer(data))\n",
    "# print(i_layer(int_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
